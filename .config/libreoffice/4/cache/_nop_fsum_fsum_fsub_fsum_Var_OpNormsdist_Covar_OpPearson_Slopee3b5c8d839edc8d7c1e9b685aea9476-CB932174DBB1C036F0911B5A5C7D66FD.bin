//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32603126
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_61, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope

.entry DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9
)
{
	.reg .pred 	%p<263>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<297>;
	.reg .f64 	%fd<1500>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd10, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd11, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	ld.param.u64 	%rd12, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd13, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	ld.param.u64 	%rd14, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	ld.param.u64 	%rd15, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	mov.b32 	%r86, %envreg3;
	mov.u32 	%r87, %ctaid.x;
	mov.u32 	%r88, %ntid.x;
	mov.u32 	%r89, %tid.x;
	add.s32 	%r90, %r89, %r86;
	mad.lo.s32 	%r1, %r88, %r87, %r90;
	setp.gt.s32 	%p1, %r1, 5;
	mov.f64 	%fd1428, 0d0000000000000000;
	mov.f64 	%fd1429, %fd1428;
	mov.f64 	%fd1430, %fd1428;
	@%p1 bra 	$L__BB0_6;

	mov.u32 	%r267, 1;
	mov.u32 	%r266, %r1;

$L__BB0_2:
	mul.wide.s32 	%rd16, %r266, 8;
	add.s64 	%rd17, %rd15, %rd16;
	add.s64 	%rd18, %rd14, %rd16;
	ld.global.f64 	%fd4, [%rd18];
	ld.global.f64 	%fd5, [%rd17];
	abs.f64 	%fd215, %fd5;
	setp.gtu.f64 	%p2, %fd215, 0d7FF0000000000000;
	@%p2 bra 	$L__BB0_5;

	abs.f64 	%fd216, %fd4;
	setp.gtu.f64 	%p3, %fd216, 0d7FF0000000000000;
	@%p3 bra 	$L__BB0_5;

	add.f64 	%fd1430, %fd1430, %fd5;
	add.f64 	%fd1429, %fd1429, %fd4;
	add.f64 	%fd1428, %fd1428, 0d3FF0000000000000;

$L__BB0_5:
	add.s32 	%r266, %r267, %r1;
	setp.lt.s32 	%p4, %r266, 6;
	setp.lt.u32 	%p5, %r267, 2;
	mov.u32 	%r267, 2;
	and.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB0_2;

$L__BB0_6:
	setp.lt.f64 	%p7, %fd1428, 0d3FF0000000000000;
	mov.f64 	%fd1448, 0d7FF8000000000207;
	mov.f64 	%fd1437, 0d3FF0000000000000;
	@%p7 bra 	$L__BB0_65;

	setp.eq.f64 	%p8, %fd1428, 0d3FF0000000000000;
	@%p8 bra 	$L__BB0_33;

	abs.f64 	%fd15, %fd1428;
	setp.gtu.f64 	%p9, %fd15, 0d7FF0000000000000;
	@%p9 bra 	$L__BB0_32;
	bra.uni 	$L__BB0_9;

$L__BB0_32:
	add.f64 	%fd1437, %fd1428, 0dBFF0000000000000;
	bra.uni 	$L__BB0_33;

$L__BB0_9:
	setp.eq.f64 	%p10, %fd1428, 0d7FF0000000000000;
	@%p10 bra 	$L__BB0_31;
	bra.uni 	$L__BB0_10;

$L__BB0_31:
	mov.f64 	%fd411, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r123}, %fd411;
	}
	setp.gt.s32 	%p33, %r123, -1;
	selp.f64 	%fd1437, 0d7FF0000000000000, 0d0000000000000000, %p33;

$L__BB0_33:
	mul.f64 	%fd34, %fd1430, %fd1437;
	mul.f64 	%fd35, %fd1429, %fd1437;
	mov.f64 	%fd1440, 0d0000000000000000;
	mov.f64 	%fd1441, %fd1440;
	@%p1 bra 	$L__BB0_39;

	mov.u32 	%r273, 1;
	mov.u32 	%r272, %r1;

$L__BB0_35:
	mul.wide.s32 	%rd21, %r272, 8;
	add.s64 	%rd22, %rd15, %rd21;
	add.s64 	%rd23, %rd14, %rd21;
	ld.global.f64 	%fd38, [%rd23];
	ld.global.f64 	%fd39, [%rd22];
	abs.f64 	%fd416, %fd39;
	setp.gtu.f64 	%p35, %fd416, 0d7FF0000000000000;
	@%p35 bra 	$L__BB0_38;

	abs.f64 	%fd417, %fd38;
	setp.gtu.f64 	%p36, %fd417, 0d7FF0000000000000;
	@%p36 bra 	$L__BB0_38;

	sub.f64 	%fd418, %fd39, %fd34;
	sub.f64 	%fd419, %fd38, %fd35;
	fma.rn.f64 	%fd1441, %fd418, %fd419, %fd1441;
	fma.rn.f64 	%fd1440, %fd418, %fd418, %fd1440;

$L__BB0_38:
	add.s32 	%r272, %r273, %r1;
	setp.lt.s32 	%p37, %r272, 6;
	setp.lt.u32 	%p38, %r273, 2;
	mov.u32 	%r273, 2;
	and.pred  	%p39, %p38, %p37;
	@%p39 bra 	$L__BB0_35;

$L__BB0_39:
	setp.eq.f64 	%p40, %fd1440, 0d0000000000000000;
	mov.f64 	%fd1448, 0d7FF8000000000214;
	mov.f64 	%fd1447, 0d3FF0000000000000;
	@%p40 bra 	$L__BB0_65;

	setp.eq.f64 	%p41, %fd1440, 0d3FF0000000000000;
	@%p41 bra 	$L__BB0_64;

	abs.f64 	%fd46, %fd1440;
	setp.gtu.f64 	%p42, %fd46, 0d7FF0000000000000;
	@%p42 bra 	$L__BB0_63;
	bra.uni 	$L__BB0_42;

$L__BB0_63:
	add.f64 	%fd1447, %fd1440, 0dBFF0000000000000;
	bra.uni 	$L__BB0_64;

$L__BB0_10:
	mov.f64 	%fd219, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd219;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd219;
	}
	and.b32  	%r95, %r94, 2147483647;
	setp.ne.s32 	%p11, %r95, 2146435072;
	setp.ne.s32 	%p12, %r93, 0;
	or.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_11;

$L__BB0_13:
	mov.f64 	%fd224, 0d3FE0000000000000;
	mul.rn.f64 	%fd225, %fd224, %fd219;
	cvt.rzi.f64.f64 	%fd226, %fd225;
	mov.f64 	%fd227, 0d4000000000000000;
	mul.rn.f64 	%fd228, %fd227, %fd226;
	sub.f64 	%fd229, %fd219, %fd228;
	abs.f64 	%fd17, %fd229;
	setp.eq.f64 	%p16, %fd1428, 0d0000000000000000;
	@%p16 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_14;

$L__BB0_30:
	setp.eq.f64 	%p32, %fd17, 0d3FF0000000000000;
	rcp.rn.f64 	%fd408, %fd1428;
	mov.f64 	%fd409, 0d0000000000000000;
	rcp.rn.f64 	%fd410, %fd409;
	selp.f64 	%fd1437, %fd408, %fd410, %p32;
	bra.uni 	$L__BB0_33;

$L__BB0_42:
	setp.eq.f64 	%p43, %fd1440, 0d7FF0000000000000;
	@%p43 bra 	$L__BB0_62;
	bra.uni 	$L__BB0_43;

$L__BB0_62:
	mov.f64 	%fd611, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r156}, %fd611;
	}
	setp.gt.s32 	%p64, %r156, -1;
	selp.f64 	%fd1447, 0d7FF0000000000000, 0d0000000000000000, %p64;

$L__BB0_64:
	mul.f64 	%fd1448, %fd1441, %fd1447;

$L__BB0_65:
	add.f64 	%fd66, %fd1448, 0d0000000000000000;
	mov.f64 	%fd1452, 0d0000000000000000;
	mov.f64 	%fd1453, %fd1452;
	mov.f64 	%fd1454, %fd1452;
	@%p1 bra 	$L__BB0_71;

	mov.u32 	%r279, 1;
	mov.u32 	%r278, %r1;

$L__BB0_67:
	mul.wide.s32 	%rd26, %r278, 8;
	add.s64 	%rd27, %rd12, %rd26;
	add.s64 	%rd28, %rd13, %rd26;
	ld.global.f64 	%fd70, [%rd28];
	ld.global.f64 	%fd71, [%rd27];
	abs.f64 	%fd618, %fd71;
	setp.gtu.f64 	%p66, %fd618, 0d7FF0000000000000;
	@%p66 bra 	$L__BB0_70;

	abs.f64 	%fd619, %fd70;
	setp.gtu.f64 	%p67, %fd619, 0d7FF0000000000000;
	@%p67 bra 	$L__BB0_70;

	add.f64 	%fd1453, %fd1453, %fd71;
	add.f64 	%fd1452, %fd1452, %fd70;
	add.f64 	%fd1454, %fd1454, 0d3FF0000000000000;

$L__BB0_70:
	add.s32 	%r278, %r279, %r1;
	setp.lt.s32 	%p68, %r278, 6;
	setp.lt.u32 	%p69, %r279, 2;
	mov.u32 	%r279, 2;
	and.pred  	%p70, %p69, %p68;
	@%p70 bra 	$L__BB0_67;

$L__BB0_71:
	setp.lt.f64 	%p71, %fd1454, 0d3FF0000000000000;
	mov.f64 	%fd1467, 0d7FF8000000000207;
	@%p71 bra 	$L__BB0_80;

	div.rn.f64 	%fd81, %fd1453, %fd1454;
	div.rn.f64 	%fd82, %fd1452, %fd1454;
	mov.f64 	%fd1461, 0d0000000000000000;
	mov.f64 	%fd1462, %fd1461;
	mov.f64 	%fd1463, %fd1461;
	@%p1 bra 	$L__BB0_78;

	mov.u32 	%r281, 1;
	mov.u32 	%r280, %r1;

$L__BB0_74:
	mul.wide.s32 	%rd29, %r280, 8;
	add.s64 	%rd30, %rd12, %rd29;
	add.s64 	%rd31, %rd13, %rd29;
	ld.global.f64 	%fd86, [%rd31];
	ld.global.f64 	%fd87, [%rd30];
	abs.f64 	%fd627, %fd87;
	setp.gtu.f64 	%p73, %fd627, 0d7FF0000000000000;
	@%p73 bra 	$L__BB0_77;

	abs.f64 	%fd628, %fd86;
	setp.gtu.f64 	%p74, %fd628, 0d7FF0000000000000;
	@%p74 bra 	$L__BB0_77;

	sub.f64 	%fd629, %fd87, %fd81;
	sub.f64 	%fd630, %fd86, %fd82;
	fma.rn.f64 	%fd1461, %fd629, %fd630, %fd1461;
	fma.rn.f64 	%fd1463, %fd629, %fd629, %fd1463;
	fma.rn.f64 	%fd1462, %fd630, %fd630, %fd1462;

$L__BB0_77:
	add.s32 	%r280, %r281, %r1;
	setp.lt.s32 	%p75, %r280, 6;
	setp.lt.u32 	%p76, %r281, 2;
	mov.u32 	%r281, 2;
	and.pred  	%p77, %p76, %p75;
	@%p77 bra 	$L__BB0_74;

$L__BB0_78:
	setp.eq.f64 	%p78, %fd1462, 0d0000000000000000;
	setp.eq.f64 	%p79, %fd1463, 0d0000000000000000;
	or.pred  	%p80, %p78, %p79;
	mov.f64 	%fd1467, 0d7FF8000000000214;
	@%p80 bra 	$L__BB0_80;

	mul.f64 	%fd632, %fd1462, %fd1463;
	sqrt.rn.f64 	%fd633, %fd632;
	div.rn.f64 	%fd1467, %fd1461, %fd633;

$L__BB0_80:
	add.f64 	%fd99, %fd1467, 0d0000000000000000;
	mov.f64 	%fd1472, 0d0000000000000000;
	mov.u32 	%r286, 0;
	mov.f64 	%fd1473, %fd1472;
	@%p1 bra 	$L__BB0_86;

	mov.u32 	%r283, 1;
	mov.u32 	%r282, %r1;

$L__BB0_82:
	cvt.s64.s32 	%rd2, %r282;
	mul.wide.s32 	%rd32, %r282, 8;
	add.s64 	%rd33, %rd10, %rd32;
	ld.global.f64 	%fd1471, [%rd33];
	abs.f64 	%fd638, %fd1471;
	setp.gtu.f64 	%p82, %fd638, 0d7FF0000000000000;
	@%p82 bra 	$L__BB0_84;

	shl.b64 	%rd34, %rd2, 3;
	add.s64 	%rd35, %rd11, %rd34;
	ld.global.f64 	%fd1470, [%rd35];
	abs.f64 	%fd639, %fd1470;
	setp.le.f64 	%p83, %fd639, 0d7FF0000000000000;
	@%p83 bra 	$L__BB0_85;

$L__BB0_84:
	add.s32 	%r286, %r286, -1;
	mov.f64 	%fd1470, 0d0000000000000000;
	mov.f64 	%fd1471, %fd1470;

$L__BB0_85:
	add.s32 	%r286, %r286, 1;
	add.f64 	%fd1473, %fd1473, %fd1471;
	add.f64 	%fd1472, %fd1472, %fd1470;
	add.s32 	%r282, %r283, %r1;
	setp.lt.s32 	%p84, %r282, 6;
	setp.lt.u32 	%p85, %r283, 2;
	mov.u32 	%r283, 2;
	and.pred  	%p86, %p85, %p84;
	@%p86 bra 	$L__BB0_82;

$L__BB0_86:
	setp.lt.s32 	%p87, %r286, 1;
	mov.f64 	%fd1478, 0d7FF8000000000207;
	@%p87 bra 	$L__BB0_94;

	cvt.rn.f64.s32 	%fd110, %r286;
	mov.f64 	%fd1477, 0d0000000000000000;
	@%p1 bra 	$L__BB0_93;

	div.rn.f64 	%fd111, %fd1473, %fd110;
	div.rn.f64 	%fd112, %fd1472, %fd110;
	mov.u32 	%r288, 1;
	mov.u32 	%r287, %r1;

$L__BB0_89:
	cvt.s64.s32 	%rd3, %r287;
	mul.wide.s32 	%rd36, %r287, 8;
	add.s64 	%rd37, %rd10, %rd36;
	ld.global.f64 	%fd114, [%rd37];
	abs.f64 	%fd645, %fd114;
	setp.gtu.f64 	%p89, %fd645, 0d7FF0000000000000;
	mov.f64 	%fd1475, %fd112;
	mov.f64 	%fd1476, %fd111;
	@%p89 bra 	$L__BB0_92;

	shl.b64 	%rd38, %rd3, 3;
	add.s64 	%rd39, %rd11, %rd38;
	ld.global.f64 	%fd115, [%rd39];
	abs.f64 	%fd646, %fd115;
	setp.gtu.f64 	%p90, %fd646, 0d7FF0000000000000;
	mov.f64 	%fd1475, %fd112;
	mov.f64 	%fd1476, %fd111;
	@%p90 bra 	$L__BB0_92;

	mov.f64 	%fd1475, %fd115;
	mov.f64 	%fd1476, %fd114;

$L__BB0_92:
	sub.f64 	%fd647, %fd1475, %fd112;
	sub.f64 	%fd648, %fd1476, %fd111;
	fma.rn.f64 	%fd1477, %fd648, %fd647, %fd1477;
	add.s32 	%r287, %r288, %r1;
	setp.lt.s32 	%p91, %r287, 6;
	setp.lt.u32 	%p92, %r288, 2;
	mov.u32 	%r288, 2;
	and.pred  	%p93, %p92, %p91;
	@%p93 bra 	$L__BB0_89;

$L__BB0_93:
	div.rn.f64 	%fd1478, %fd1477, %fd110;

$L__BB0_94:
	setp.gt.s32 	%p94, %r1, 4;
	mov.f64 	%fd1479, 0d8000000000000000;
	@%p94 bra 	$L__BB0_97;

	cvt.s64.s32 	%rd67, %r1;
	ld.param.u64 	%rd65, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3];
	shl.b64 	%rd40, %rd67, 3;
	add.s64 	%rd41, %rd65, %rd40;
	ld.global.f64 	%fd122, [%rd41];
	abs.f64 	%fd651, %fd122;
	setp.gtu.f64 	%p95, %fd651, 0d7FF0000000000000;
	@%p95 bra 	$L__BB0_97;

	mul.f64 	%fd1479, %fd122, 0dBFE6A09E667F3BCC;

$L__BB0_97:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r167}, %fd1479; 
	}
	// end inline asm
	setp.lt.s32 	%p96, %r167, 1072168960;
	@%p96 bra 	$L__BB0_102;
	bra.uni 	$L__BB0_98;

$L__BB0_102:
	abs.f64 	%fd899, %fd1479;
	mov.f64 	%fd900, 0d3D47088FDB46FA5F;
	mov.f64 	%fd901, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd902, %fd901, %fd899, %fd900;
	mov.f64 	%fd903, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd904, %fd902, %fd899, %fd903;
	mov.f64 	%fd905, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd906, %fd904, %fd899, %fd905;
	mov.f64 	%fd907, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd908, %fd906, %fd899, %fd907;
	mov.f64 	%fd909, 0d3E2485EE67835925;
	fma.rn.f64 	%fd910, %fd908, %fd899, %fd909;
	mov.f64 	%fd911, 0dBE476DB45919F583;
	fma.rn.f64 	%fd912, %fd910, %fd899, %fd911;
	mov.f64 	%fd913, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd914, %fd912, %fd899, %fd913;
	mov.f64 	%fd915, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd916, %fd914, %fd899, %fd915;
	mov.f64 	%fd917, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd918, %fd916, %fd899, %fd917;
	mov.f64 	%fd919, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd920, %fd918, %fd899, %fd919;
	mov.f64 	%fd921, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd922, %fd920, %fd899, %fd921;
	mov.f64 	%fd923, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd924, %fd922, %fd899, %fd923;
	mov.f64 	%fd925, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd926, %fd924, %fd899, %fd925;
	mov.f64 	%fd927, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd928, %fd926, %fd899, %fd927;
	mov.f64 	%fd929, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd930, %fd928, %fd899, %fd929;
	mov.f64 	%fd931, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd932, %fd930, %fd899, %fd931;
	mov.f64 	%fd933, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd934, %fd932, %fd899, %fd933;
	mov.f64 	%fd935, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd936, %fd934, %fd899, %fd935;
	mov.f64 	%fd937, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd938, %fd936, %fd899, %fd937;
	mov.f64 	%fd939, 0dBF9396685912A453;
	fma.rn.f64 	%fd940, %fd938, %fd899, %fd939;
	mov.f64 	%fd941, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd942, %fd940, %fd899, %fd941;
	mov.f64 	%fd943, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd944, %fd942, %fd899, %fd943;
	mov.f64 	%fd945, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd946, %fd944, %fd899, %fd945;
	fma.rn.f64 	%fd947, %fd946, %fd899, %fd899;
	sub.f64 	%fd948, %fd899, %fd947;
	fma.rn.f64 	%fd949, %fd946, %fd899, %fd948;
	neg.f64 	%fd950, %fd947;
	neg.f64 	%fd951, %fd949;
	cvt.rn.f32.f64 	%f7, %fd947;
	mul.f32 	%f8, %f7, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f9, %f8;
	cvt.f64.f32 	%fd952, %f9;
	neg.f64 	%fd953, %fd952;
	mov.f64 	%fd954, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd955, %fd953, %fd954, %fd950;
	mov.f64 	%fd956, 0d3E928A27F89B6999;
	mov.f64 	%fd957, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd958, %fd957, %fd955, %fd956;
	mov.f64 	%fd959, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd960, %fd958, %fd955, %fd959;
	mov.f64 	%fd961, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd962, %fd960, %fd955, %fd961;
	mov.f64 	%fd963, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd964, %fd962, %fd955, %fd963;
	mov.f64 	%fd965, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd966, %fd964, %fd955, %fd965;
	mov.f64 	%fd967, 0d3F811111111173C4;
	fma.rn.f64 	%fd968, %fd966, %fd955, %fd967;
	mov.f64 	%fd969, 0d3FA555555555211A;
	fma.rn.f64 	%fd970, %fd968, %fd955, %fd969;
	mov.f64 	%fd971, 0d3FC5555555555540;
	fma.rn.f64 	%fd972, %fd970, %fd955, %fd971;
	mov.f64 	%fd973, 0d3FE0000000000005;
	fma.rn.f64 	%fd974, %fd972, %fd955, %fd973;
	mul.f64 	%fd975, %fd955, %fd974;
	fma.rn.f64 	%fd976, %fd975, %fd955, %fd951;
	add.f64 	%fd977, %fd955, %fd976;
	ex2.approx.ftz.f32 	%f10, %f9;
	cvt.f64.f32 	%fd978, %f10;
	mov.f64 	%fd979, 0d3FF0000000000000;
	sub.f64 	%fd980, %fd979, %fd978;
	neg.f64 	%fd981, %fd977;
	fma.rn.f64 	%fd982, %fd981, %fd978, %fd980;
	setp.ge.f64 	%p101, %fd899, 0d4017AFB48DC96626;
	selp.f64 	%fd983, 0d3FF0000000000000, %fd982, %p101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r190, %temp}, %fd983;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r191}, %fd983;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r192}, %fd1479;
	}
	and.b32  	%r193, %r192, -2147483648;
	or.b32  	%r194, %r191, %r193;
	mov.b64 	%fd984, {%r190, %r194};
	sub.f64 	%fd1480, %fd979, %fd984;
	bra.uni 	$L__BB0_103;

$L__BB0_98:
	setp.gt.f64 	%p97, %fd1479, 0d403B4CCCCCCCCCCD;
	mov.f64 	%fd1480, 0d0000000000000000;
	@%p97 bra 	$L__BB0_103;

	setp.lt.s32 	%p98, %r167, 1075052544;
	@%p98 bra 	$L__BB0_101;
	bra.uni 	$L__BB0_100;

$L__BB0_101:
	mov.f64 	%fd772, 0d3FE20DD7452FBC22;
	mov.f64 	%fd774, 0d401FD453E105E9A2;
	// begin inline asm
	fma.rn.f64 	%fd771, %fd772, %fd1479, %fd774;
	// end inline asm
	mov.f64 	%fd778, 0d404B26245B951FB4;
	// begin inline asm
	fma.rn.f64 	%fd775, %fd771, %fd1479, %fd778;
	// end inline asm
	mov.f64 	%fd782, 0d406C7835DC0F1F49;
	// begin inline asm
	fma.rn.f64 	%fd779, %fd775, %fd1479, %fd782;
	// end inline asm
	mov.f64 	%fd786, 0d4083AFA471E5C766;
	// begin inline asm
	fma.rn.f64 	%fd783, %fd779, %fd1479, %fd786;
	// end inline asm
	mov.f64 	%fd790, 0d4091FB514824F49F;
	// begin inline asm
	fma.rn.f64 	%fd787, %fd783, %fd1479, %fd790;
	// end inline asm
	mov.f64 	%fd794, 0d409450DDEE8272BB;
	// begin inline asm
	fma.rn.f64 	%fd791, %fd787, %fd1479, %fd794;
	// end inline asm
	mov.f64 	%fd798, 0d4086B952E4ECBC50;
	// begin inline asm
	fma.rn.f64 	%fd795, %fd791, %fd1479, %fd798;
	// end inline asm
	add.f64 	%fd800, %fd1479, 0d402C35442E99E667;
	mov.f64 	%fd802, 0d40582F68071A079D;
	// begin inline asm
	fma.rn.f64 	%fd799, %fd800, %fd1479, %fd802;
	// end inline asm
	mov.f64 	%fd806, 0d4079ABD39A029DAA;
	// begin inline asm
	fma.rn.f64 	%fd803, %fd799, %fd1479, %fd806;
	// end inline asm
	mov.f64 	%fd810, 0d409230CA327093FD;
	// begin inline asm
	fma.rn.f64 	%fd807, %fd803, %fd1479, %fd810;
	// end inline asm
	mov.f64 	%fd814, 0d40A174FAB33B54A7;
	// begin inline asm
	fma.rn.f64 	%fd811, %fd807, %fd1479, %fd814;
	// end inline asm
	mov.f64 	%fd818, 0d40A601508230F980;
	// begin inline asm
	fma.rn.f64 	%fd815, %fd811, %fd1479, %fd818;
	// end inline asm
	mov.f64 	%fd822, 0d40A091785EC9331E;
	// begin inline asm
	fma.rn.f64 	%fd819, %fd815, %fd1479, %fd822;
	// end inline asm
	mov.f64 	%fd826, 0d4086B952E52F3622;
	// begin inline asm
	fma.rn.f64 	%fd823, %fd819, %fd1479, %fd826;
	// end inline asm
	div.rn.f64 	%fd892, %fd795, %fd823;
	mul.rn.f64 	%fd893, %fd1479, %fd1479;
	neg.f64 	%fd834, %fd893;
	// begin inline asm
	fma.rn.f64 	%fd827, %fd1479, %fd1479, %fd834;
	// end inline asm
	mov.f64 	%fd894, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd895, %fd834, %fd894;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r181}, %fd895;
	}
	and.b32  	%r182, %r181, -2147483648;
	mov.f64 	%fd878, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd878;
	}
	or.b32  	%r184, %r183, %r182;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd878;
	}
	mov.b64 	%fd896, {%r185, %r184};
	add.rz.f64 	%fd897, %fd895, %fd896;
	cvt.rzi.f64.f64 	%fd836, %fd897;
	cvt.rzi.s32.f64 	%r186, %fd836;
	mov.f64 	%fd833, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd831, %fd836, %fd833, %fd834;
	// end inline asm
	mov.f64 	%fd837, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd835, %fd836, %fd837, %fd831;
	// end inline asm
	setp.lt.s32 	%p100, %r186, -1020;
	selp.f64 	%fd898, 0d3C90000000000000, 0d4000000000000000, %p100;
	mov.f64 	%fd840, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd842, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd839, %fd840, %fd835, %fd842;
	// end inline asm
	mov.f64 	%fd846, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd843, %fd839, %fd835, %fd846;
	// end inline asm
	mov.f64 	%fd850, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd847, %fd843, %fd835, %fd850;
	// end inline asm
	mov.f64 	%fd854, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd851, %fd847, %fd835, %fd854;
	// end inline asm
	mov.f64 	%fd858, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd855, %fd851, %fd835, %fd858;
	// end inline asm
	mov.f64 	%fd862, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd859, %fd855, %fd835, %fd862;
	// end inline asm
	mov.f64 	%fd866, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd863, %fd859, %fd835, %fd866;
	// end inline asm
	mov.f64 	%fd870, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd867, %fd863, %fd835, %fd870;
	// end inline asm
	mov.f64 	%fd874, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd871, %fd867, %fd835, %fd874;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd875, %fd871, %fd835, %fd878;
	// end inline asm
	mul.rn.f64 	%fd880, %fd875, %fd835;
	// begin inline asm
	fma.rn.f64 	%fd879, %fd880, %fd835, %fd835;
	// end inline asm
	shl.b32 	%r187, %r186, 20;
	add.s32 	%r188, %r187, 57671680;
	selp.b32 	%r189, %r188, %r187, %p100;
	add.s32 	%r180, %r189, 1071644672;
	mov.u32 	%r179, 0;
	// begin inline asm
	mov.b64 	%fd883, {%r179, %r180};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd884, %fd879, %fd883, %fd883;
	// end inline asm
	mul.rn.f64 	%fd891, %fd884, %fd898;
	neg.f64 	%fd890, %fd891;
	// begin inline asm
	fma.rn.f64 	%fd888, %fd827, %fd890, %fd891;
	// end inline asm
	mul.rn.f64 	%fd1480, %fd892, %fd888;
	bra.uni 	$L__BB0_103;

$L__BB0_100:
	rcp.rn.f64 	%fd763, %fd1479;
	mul.rn.f64 	%fd696, %fd763, %fd763;
	mov.f64 	%fd655, 0dC1186DF84479631D;
	mov.f64 	%fd657, 0d41019A6E9A7FFBB8;
	// begin inline asm
	fma.rn.f64 	%fd654, %fd655, %fd696, %fd657;
	// end inline asm
	mov.f64 	%fd661, 0dC0DB040BE3D5CA18;
	// begin inline asm
	fma.rn.f64 	%fd658, %fd654, %fd696, %fd661;
	// end inline asm
	mov.f64 	%fd665, 0d40B012760EE009A0;
	// begin inline asm
	fma.rn.f64 	%fd662, %fd658, %fd696, %fd665;
	// end inline asm
	mov.f64 	%fd669, 0dC082587AE4008D0E;
	// begin inline asm
	fma.rn.f64 	%fd666, %fd662, %fd696, %fd669;
	// end inline asm
	mov.f64 	%fd673, 0d4056DF5D938ACAFE;
	// begin inline asm
	fma.rn.f64 	%fd670, %fd666, %fd696, %fd673;
	// end inline asm
	mov.f64 	%fd677, 0dC030A8D46D765681;
	// begin inline asm
	fma.rn.f64 	%fd674, %fd670, %fd696, %fd677;
	// end inline asm
	mov.f64 	%fd681, 0d400D9EAE0C665C75;
	// begin inline asm
	fma.rn.f64 	%fd678, %fd674, %fd696, %fd681;
	// end inline asm
	mov.f64 	%fd685, 0dBFF0ECF9C8880942;
	// begin inline asm
	fma.rn.f64 	%fd682, %fd678, %fd696, %fd685;
	// end inline asm
	mov.f64 	%fd689, 0d3FDB14C2F82A33F7;
	// begin inline asm
	fma.rn.f64 	%fd686, %fd682, %fd696, %fd689;
	// end inline asm
	mov.f64 	%fd693, 0dBFD20DD75042844F;
	// begin inline asm
	fma.rn.f64 	%fd690, %fd686, %fd696, %fd693;
	// end inline asm
	mov.f64 	%fd697, 0d3FE20DD750429B6B;
	// begin inline asm
	fma.rn.f64 	%fd694, %fd690, %fd696, %fd697;
	// end inline asm
	mul.rn.f64 	%fd764, %fd1479, %fd1479;
	neg.f64 	%fd705, %fd764;
	// begin inline asm
	fma.rn.f64 	%fd698, %fd1479, %fd1479, %fd705;
	// end inline asm
	mov.f64 	%fd765, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd766, %fd705, %fd765;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r170}, %fd766;
	}
	and.b32  	%r171, %r170, -2147483648;
	mov.f64 	%fd749, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r172}, %fd749;
	}
	or.b32  	%r173, %r172, %r171;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r174, %temp}, %fd749;
	}
	mov.b64 	%fd767, {%r174, %r173};
	add.rz.f64 	%fd768, %fd766, %fd767;
	cvt.rzi.f64.f64 	%fd707, %fd768;
	cvt.rzi.s32.f64 	%r175, %fd707;
	mov.f64 	%fd704, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd702, %fd707, %fd704, %fd705;
	// end inline asm
	mov.f64 	%fd708, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd706, %fd707, %fd708, %fd702;
	// end inline asm
	setp.lt.s32 	%p99, %r175, -1020;
	selp.f64 	%fd769, 0d3C90000000000000, 0d4000000000000000, %p99;
	mov.f64 	%fd711, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd713, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd710, %fd711, %fd706, %fd713;
	// end inline asm
	mov.f64 	%fd717, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd714, %fd710, %fd706, %fd717;
	// end inline asm
	mov.f64 	%fd721, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd718, %fd714, %fd706, %fd721;
	// end inline asm
	mov.f64 	%fd725, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd722, %fd718, %fd706, %fd725;
	// end inline asm
	mov.f64 	%fd729, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd726, %fd722, %fd706, %fd729;
	// end inline asm
	mov.f64 	%fd733, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd730, %fd726, %fd706, %fd733;
	// end inline asm
	mov.f64 	%fd737, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd734, %fd730, %fd706, %fd737;
	// end inline asm
	mov.f64 	%fd741, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd738, %fd734, %fd706, %fd741;
	// end inline asm
	mov.f64 	%fd745, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd742, %fd738, %fd706, %fd745;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd746, %fd742, %fd706, %fd749;
	// end inline asm
	mul.rn.f64 	%fd751, %fd746, %fd706;
	// begin inline asm
	fma.rn.f64 	%fd750, %fd751, %fd706, %fd706;
	// end inline asm
	shl.b32 	%r176, %r175, 20;
	add.s32 	%r177, %r176, 57671680;
	selp.b32 	%r178, %r177, %r176, %p99;
	add.s32 	%r169, %r178, 1071644672;
	mov.u32 	%r168, 0;
	// begin inline asm
	mov.b64 	%fd754, {%r168, %r169};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd755, %fd750, %fd754, %fd754;
	// end inline asm
	mul.rn.f64 	%fd762, %fd755, %fd769;
	neg.f64 	%fd761, %fd762;
	// begin inline asm
	fma.rn.f64 	%fd759, %fd698, %fd761, %fd762;
	// end inline asm
	mul.rn.f64 	%fd770, %fd694, %fd763;
	mul.rn.f64 	%fd1480, %fd770, %fd759;

$L__BB0_103:
	cvt.s64.s32 	%rd66, %r1;
	ld.param.u64 	%rd62, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2];
	fma.rn.f64 	%fd129, %fd1480, 0d3FE0000000000000, 0d0000000000000000;
	mov.f64 	%fd1483, 0d0000000000000000;
	shl.b64 	%rd42, %rd66, 3;
	add.s64 	%rd4, %rd62, %rd42;
	mov.f64 	%fd1484, %fd1483;
	@%p94 bra 	$L__BB0_105;

	ld.global.f64 	%fd987, [%rd4];
	abs.f64 	%fd988, %fd987;
	setp.gtu.f64 	%p103, %fd988, 0d7FF0000000000000;
	add.f64 	%fd989, %fd987, 0d0000000000000000;
	selp.f64 	%fd1483, 0d0000000000000000, %fd989, %p103;
	selp.f64 	%fd1484, 0d0000000000000000, 0d3FF0000000000000, %p103;

$L__BB0_105:
	ld.param.u64 	%rd63, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1];
	add.s64 	%rd5, %rd63, %rd42;
	@%p94 bra 	$L__BB0_107;

	ld.global.f64 	%fd990, [%rd5];
	abs.f64 	%fd991, %fd990;
	setp.gtu.f64 	%p105, %fd991, 0d7FF0000000000000;
	add.f64 	%fd992, %fd1483, %fd990;
	selp.f64 	%fd1483, %fd1483, %fd992, %p105;
	add.f64 	%fd993, %fd1484, 0d3FF0000000000000;
	selp.f64 	%fd1484, %fd1484, %fd993, %p105;

$L__BB0_107:
	setp.eq.f64 	%p106, %fd1484, 0d3FF0000000000000;
	mov.f64 	%fd1488, 0d3FF0000000000000;
	@%p106 bra 	$L__BB0_133;

	abs.f64 	%fd138, %fd1484;
	setp.gtu.f64 	%p107, %fd138, 0d7FF0000000000000;
	@%p107 bra 	$L__BB0_132;
	bra.uni 	$L__BB0_109;

$L__BB0_132:
	add.f64 	%fd1488, %fd1484, 0dBFF0000000000000;
	bra.uni 	$L__BB0_133;

$L__BB0_109:
	setp.eq.f64 	%p108, %fd1484, 0d7FF0000000000000;
	@%p108 bra 	$L__BB0_131;
	bra.uni 	$L__BB0_110;

$L__BB0_131:
	mov.f64 	%fd1187, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r225}, %fd1187;
	}
	setp.gt.s32 	%p131, %r225, -1;
	selp.f64 	%fd1488, 0d7FF0000000000000, 0d0000000000000000, %p131;

$L__BB0_133:
	mul.f64 	%fd157, %fd1483, %fd1488;
	mov.f64 	%fd1490, 0d0000000000000000;
	@%p94 bra 	$L__BB0_136;

	ld.global.f64 	%fd158, [%rd4];
	abs.f64 	%fd1190, %fd158;
	setp.gtu.f64 	%p133, %fd1190, 0d7FF0000000000000;
	@%p133 bra 	$L__BB0_136;

	sub.f64 	%fd1191, %fd158, %fd157;
	fma.rn.f64 	%fd1490, %fd1191, %fd1191, 0d0000000000000000;

$L__BB0_136:
	@%p94 bra 	$L__BB0_139;

	ld.global.f64 	%fd161, [%rd5];
	abs.f64 	%fd1192, %fd161;
	setp.gtu.f64 	%p135, %fd1192, 0d7FF0000000000000;
	@%p135 bra 	$L__BB0_139;

	sub.f64 	%fd1193, %fd161, %fd157;
	fma.rn.f64 	%fd1490, %fd1193, %fd1193, %fd1490;

$L__BB0_139:
	setp.le.f64 	%p136, %fd1484, 0d3FF0000000000000;
	mov.f64 	%fd1495, 0d7FF8000000000214;
	mov.f64 	%fd1494, 0d3FF0000000000000;
	@%p136 bra 	$L__BB0_167;

	add.f64 	%fd164, %fd1484, 0dBFF0000000000000;
	setp.eq.f64 	%p137, %fd164, 0d3FF0000000000000;
	@%p137 bra 	$L__BB0_166;

	abs.f64 	%fd165, %fd164;
	setp.gtu.f64 	%p138, %fd165, 0d7FF0000000000000;
	@%p138 bra 	$L__BB0_165;
	bra.uni 	$L__BB0_142;

$L__BB0_165:
	add.f64 	%fd1494, %fd164, 0dBFF0000000000000;
	bra.uni 	$L__BB0_166;

$L__BB0_142:
	setp.eq.f64 	%p139, %fd164, 0d7FF0000000000000;
	@%p139 bra 	$L__BB0_164;
	bra.uni 	$L__BB0_143;

$L__BB0_164:
	mov.f64 	%fd1388, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r256}, %fd1388;
	}
	setp.gt.s32 	%p162, %r256, -1;
	selp.f64 	%fd1494, 0d7FF0000000000000, 0d0000000000000000, %p162;

$L__BB0_166:
	mul.f64 	%fd1495, %fd1490, %fd1494;

$L__BB0_167:
	setp.gt.f64 	%p163, %fd129, 0d0000000000000000;
	setp.lt.f64 	%p164, %fd1495, 0d0000000000000000;
	and.pred  	%p165, %p163, %p164;
	@%p165 bra 	$L__BB0_169;

	setp.geu.f64 	%p166, %fd129, 0d0000000000000000;
	setp.leu.f64 	%p167, %fd1495, 0d0000000000000000;
	or.pred  	%p168, %p166, %p167;
	@%p168 bra 	$L__BB0_178;

$L__BB0_169:
	neg.f64 	%fd186, %fd129;
	setp.eq.f64 	%p169, %fd1495, %fd186;
	mov.f64 	%fd1496, 0d0000000000000000;
	@%p169 bra 	$L__BB0_179;

	setp.eq.f64 	%p170, %fd1495, 0d0000000000000000;
	setp.eq.f64 	%p171, %fd129, 0d8000000000000000;
	or.pred  	%p172, %p171, %p170;
	@%p172 bra 	$L__BB0_178;

	add.f64 	%fd1390, %fd129, %fd1495;
	abs.f64 	%fd187, %fd1390;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r257}, %fd187;
	}
	and.b32  	%r258, %r257, 2146435072;
	setp.eq.s32 	%p173, %r258, 2146435072;
	@%p173 bra 	$L__BB0_178;

	abs.f64 	%fd188, %fd1495;
	mul.f64 	%fd1391, %fd188, 0d3D30000000000000;
	setp.gt.f64 	%p174, %fd187, %fd1391;
	@%p174 bra 	$L__BB0_178;

	abs.f64 	%fd189, %fd186;
	mul.f64 	%fd1392, %fd189, 0d3D30000000000000;
	setp.gt.f64 	%p175, %fd187, %fd1392;
	@%p175 bra 	$L__BB0_178;

	setp.gtu.f64 	%p176, %fd187, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd48, %fd187;
	setp.gt.s64 	%p177, %rd48, 9007199254740991;
	or.pred  	%p178, %p176, %p177;
	@%p178 bra 	$L__BB0_177;

	setp.gtu.f64 	%p179, %fd188, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd49, %fd188;
	setp.gt.s64 	%p180, %rd49, 9007199254740991;
	or.pred  	%p181, %p179, %p180;
	@%p181 bra 	$L__BB0_177;

	setp.le.f64 	%p182, %fd189, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd50, %fd189;
	setp.lt.s64 	%p183, %rd50, 9007199254740992;
	and.pred  	%p184, %p182, %p183;
	@%p184 bra 	$L__BB0_178;

$L__BB0_177:
	mul.f64 	%fd1394, %fd188, 0d3CF0000000000000;
	setp.lt.f64 	%p185, %fd187, %fd1394;
	mul.f64 	%fd1395, %fd189, 0d3CF0000000000000;
	setp.lt.f64 	%p186, %fd187, %fd1395;
	and.pred  	%p187, %p185, %p186;
	@%p187 bra 	$L__BB0_179;

$L__BB0_178:
	add.f64 	%fd1496, %fd129, %fd1495;

$L__BB0_179:
	setp.lt.f64 	%p188, %fd1478, 0d0000000000000000;
	setp.lt.f64 	%p189, %fd1496, 0d0000000000000000;
	and.pred  	%p190, %p188, %p189;
	@%p190 bra 	$L__BB0_181;

	setp.leu.f64 	%p191, %fd1496, 0d0000000000000000;
	setp.leu.f64 	%p192, %fd1478, 0d0000000000000000;
	or.pred  	%p193, %p192, %p191;
	@%p193 bra 	$L__BB0_190;

$L__BB0_181:
	setp.eq.f64 	%p194, %fd1496, %fd1478;
	mov.f64 	%fd1497, 0d0000000000000000;
	@%p194 bra 	$L__BB0_191;

	setp.eq.f64 	%p195, %fd1496, 0d0000000000000000;
	setp.eq.f64 	%p196, %fd1478, 0d0000000000000000;
	or.pred  	%p197, %p196, %p195;
	@%p197 bra 	$L__BB0_190;

	sub.f64 	%fd1397, %fd1496, %fd1478;
	abs.f64 	%fd192, %fd1397;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r259}, %fd192;
	}
	and.b32  	%r260, %r259, 2146435072;
	setp.eq.s32 	%p198, %r260, 2146435072;
	@%p198 bra 	$L__BB0_190;

	abs.f64 	%fd193, %fd1496;
	mul.f64 	%fd1398, %fd193, 0d3D30000000000000;
	setp.gt.f64 	%p199, %fd192, %fd1398;
	@%p199 bra 	$L__BB0_190;

	abs.f64 	%fd194, %fd1478;
	mul.f64 	%fd1399, %fd194, 0d3D30000000000000;
	setp.gt.f64 	%p200, %fd192, %fd1399;
	@%p200 bra 	$L__BB0_190;

	setp.gtu.f64 	%p201, %fd192, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd51, %fd192;
	setp.gt.s64 	%p202, %rd51, 9007199254740991;
	or.pred  	%p203, %p201, %p202;
	@%p203 bra 	$L__BB0_189;

	setp.gtu.f64 	%p204, %fd193, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd52, %fd193;
	setp.gt.s64 	%p205, %rd52, 9007199254740991;
	or.pred  	%p206, %p204, %p205;
	@%p206 bra 	$L__BB0_189;

	setp.le.f64 	%p207, %fd194, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd53, %fd194;
	setp.lt.s64 	%p208, %rd53, 9007199254740992;
	and.pred  	%p209, %p207, %p208;
	@%p209 bra 	$L__BB0_190;

$L__BB0_189:
	mul.f64 	%fd1401, %fd193, 0d3CF0000000000000;
	setp.lt.f64 	%p210, %fd192, %fd1401;
	mul.f64 	%fd1402, %fd194, 0d3CF0000000000000;
	setp.lt.f64 	%p211, %fd192, %fd1402;
	and.pred  	%p212, %p210, %p211;
	@%p212 bra 	$L__BB0_191;

$L__BB0_190:
	sub.f64 	%fd1497, %fd1496, %fd1478;

$L__BB0_191:
	setp.gt.f64 	%p213, %fd99, 0d0000000000000000;
	setp.lt.f64 	%p214, %fd1497, 0d0000000000000000;
	and.pred  	%p215, %p213, %p214;
	@%p215 bra 	$L__BB0_193;

	setp.geu.f64 	%p216, %fd99, 0d0000000000000000;
	setp.leu.f64 	%p217, %fd1497, 0d0000000000000000;
	or.pred  	%p218, %p216, %p217;
	@%p218 bra 	$L__BB0_202;

$L__BB0_193:
	neg.f64 	%fd197, %fd99;
	setp.eq.f64 	%p219, %fd1497, %fd197;
	mov.f64 	%fd1498, 0d0000000000000000;
	@%p219 bra 	$L__BB0_203;

	setp.eq.f64 	%p220, %fd1497, 0d0000000000000000;
	setp.eq.f64 	%p221, %fd99, 0d8000000000000000;
	or.pred  	%p222, %p221, %p220;
	@%p222 bra 	$L__BB0_202;

	add.f64 	%fd1404, %fd99, %fd1497;
	abs.f64 	%fd198, %fd1404;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r261}, %fd198;
	}
	and.b32  	%r262, %r261, 2146435072;
	setp.eq.s32 	%p223, %r262, 2146435072;
	@%p223 bra 	$L__BB0_202;

	abs.f64 	%fd199, %fd1497;
	mul.f64 	%fd1405, %fd199, 0d3D30000000000000;
	setp.gt.f64 	%p224, %fd198, %fd1405;
	@%p224 bra 	$L__BB0_202;

	abs.f64 	%fd200, %fd197;
	mul.f64 	%fd1406, %fd200, 0d3D30000000000000;
	setp.gt.f64 	%p225, %fd198, %fd1406;
	@%p225 bra 	$L__BB0_202;

	setp.gtu.f64 	%p226, %fd198, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd54, %fd198;
	setp.gt.s64 	%p227, %rd54, 9007199254740991;
	or.pred  	%p228, %p226, %p227;
	@%p228 bra 	$L__BB0_201;

	setp.gtu.f64 	%p229, %fd199, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd55, %fd199;
	setp.gt.s64 	%p230, %rd55, 9007199254740991;
	or.pred  	%p231, %p229, %p230;
	@%p231 bra 	$L__BB0_201;

	setp.le.f64 	%p232, %fd200, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd56, %fd200;
	setp.lt.s64 	%p233, %rd56, 9007199254740992;
	and.pred  	%p234, %p232, %p233;
	@%p234 bra 	$L__BB0_202;

$L__BB0_201:
	mul.f64 	%fd1408, %fd199, 0d3CF0000000000000;
	setp.lt.f64 	%p235, %fd198, %fd1408;
	mul.f64 	%fd1409, %fd200, 0d3CF0000000000000;
	setp.lt.f64 	%p236, %fd198, %fd1409;
	and.pred  	%p237, %p235, %p236;
	@%p237 bra 	$L__BB0_203;

$L__BB0_202:
	add.f64 	%fd1498, %fd99, %fd1497;

$L__BB0_203:
	setp.gt.f64 	%p238, %fd66, 0d0000000000000000;
	setp.lt.f64 	%p239, %fd1498, 0d0000000000000000;
	and.pred  	%p240, %p238, %p239;
	@%p240 bra 	$L__BB0_205;

	setp.geu.f64 	%p241, %fd66, 0d0000000000000000;
	setp.leu.f64 	%p242, %fd1498, 0d0000000000000000;
	or.pred  	%p243, %p241, %p242;
	@%p243 bra 	$L__BB0_214;

$L__BB0_205:
	neg.f64 	%fd203, %fd66;
	setp.eq.f64 	%p244, %fd1498, %fd203;
	mov.f64 	%fd1499, 0d0000000000000000;
	@%p244 bra 	$L__BB0_215;

	setp.eq.f64 	%p245, %fd1498, 0d0000000000000000;
	setp.eq.f64 	%p246, %fd66, 0d8000000000000000;
	or.pred  	%p247, %p246, %p245;
	@%p247 bra 	$L__BB0_214;

	add.f64 	%fd1411, %fd66, %fd1498;
	abs.f64 	%fd204, %fd1411;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r263}, %fd204;
	}
	and.b32  	%r264, %r263, 2146435072;
	setp.eq.s32 	%p248, %r264, 2146435072;
	@%p248 bra 	$L__BB0_214;

	abs.f64 	%fd205, %fd1498;
	mul.f64 	%fd1412, %fd205, 0d3D30000000000000;
	setp.gt.f64 	%p249, %fd204, %fd1412;
	@%p249 bra 	$L__BB0_214;

	abs.f64 	%fd206, %fd203;
	mul.f64 	%fd1413, %fd206, 0d3D30000000000000;
	setp.gt.f64 	%p250, %fd204, %fd1413;
	@%p250 bra 	$L__BB0_214;

	setp.gtu.f64 	%p251, %fd204, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd57, %fd204;
	setp.gt.s64 	%p252, %rd57, 9007199254740991;
	or.pred  	%p253, %p251, %p252;
	@%p253 bra 	$L__BB0_213;

	setp.gtu.f64 	%p254, %fd205, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd58, %fd205;
	setp.gt.s64 	%p255, %rd58, 9007199254740991;
	or.pred  	%p256, %p254, %p255;
	@%p256 bra 	$L__BB0_213;

	setp.le.f64 	%p257, %fd206, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd59, %fd206;
	setp.lt.s64 	%p258, %rd59, 9007199254740992;
	and.pred  	%p259, %p257, %p258;
	@%p259 bra 	$L__BB0_214;

$L__BB0_213:
	mul.f64 	%fd1415, %fd205, 0d3CF0000000000000;
	setp.lt.f64 	%p260, %fd204, %fd1415;
	mul.f64 	%fd1416, %fd206, 0d3CF0000000000000;
	setp.lt.f64 	%p261, %fd204, %fd1416;
	and.pred  	%p262, %p260, %p261;
	@%p262 bra 	$L__BB0_215;

$L__BB0_214:
	add.f64 	%fd1499, %fd66, %fd1498;

$L__BB0_215:
	ld.param.u64 	%rd64, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0];
	add.s64 	%rd61, %rd64, %rd42;
	st.global.f64 	[%rd61], %fd1499;
	ret;

$L__BB0_110:
	mov.f64 	%fd995, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r195, %temp}, %fd995;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd995;
	}
	and.b32  	%r197, %r196, 2147483647;
	setp.ne.s32 	%p109, %r197, 2146435072;
	setp.ne.s32 	%p110, %r195, 0;
	or.pred  	%p111, %p110, %p109;
	@%p111 bra 	$L__BB0_113;
	bra.uni 	$L__BB0_111;

$L__BB0_113:
	mov.f64 	%fd1000, 0d3FE0000000000000;
	mul.rn.f64 	%fd1001, %fd1000, %fd995;
	cvt.rzi.f64.f64 	%fd1002, %fd1001;
	mov.f64 	%fd1003, 0d4000000000000000;
	mul.rn.f64 	%fd1004, %fd1003, %fd1002;
	sub.f64 	%fd1005, %fd995, %fd1004;
	abs.f64 	%fd140, %fd1005;
	setp.eq.f64 	%p114, %fd1484, 0d0000000000000000;
	@%p114 bra 	$L__BB0_130;
	bra.uni 	$L__BB0_114;

$L__BB0_130:
	setp.eq.f64 	%p130, %fd140, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1184, %fd1484;
	mov.f64 	%fd1185, 0d0000000000000000;
	rcp.rn.f64 	%fd1186, %fd1185;
	selp.f64 	%fd1488, %fd1184, %fd1186, %p130;
	bra.uni 	$L__BB0_133;

$L__BB0_143:
	mov.f64 	%fd1196, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r226, %temp}, %fd1196;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r227}, %fd1196;
	}
	and.b32  	%r228, %r227, 2147483647;
	setp.ne.s32 	%p140, %r228, 2146435072;
	setp.ne.s32 	%p141, %r226, 0;
	or.pred  	%p142, %p141, %p140;
	@%p142 bra 	$L__BB0_146;
	bra.uni 	$L__BB0_144;

$L__BB0_146:
	mov.f64 	%fd1201, 0d3FE0000000000000;
	mul.rn.f64 	%fd1202, %fd1201, %fd1196;
	cvt.rzi.f64.f64 	%fd1203, %fd1202;
	mov.f64 	%fd1204, 0d4000000000000000;
	mul.rn.f64 	%fd1205, %fd1204, %fd1203;
	sub.f64 	%fd1206, %fd1196, %fd1205;
	abs.f64 	%fd167, %fd1206;
	setp.eq.f64 	%p145, %fd164, 0d0000000000000000;
	@%p145 bra 	$L__BB0_163;
	bra.uni 	$L__BB0_147;

$L__BB0_163:
	setp.eq.f64 	%p161, %fd167, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1385, %fd164;
	mov.f64 	%fd1386, 0d0000000000000000;
	rcp.rn.f64 	%fd1387, %fd1386;
	selp.f64 	%fd1494, %fd1385, %fd1387, %p161;
	bra.uni 	$L__BB0_166;

$L__BB0_111:
	setp.eq.f64 	%p112, %fd1484, 0dBFF0000000000000;
	@%p112 bra 	$L__BB0_133;

	setp.gt.f64 	%p113, %fd138, 0d3FF0000000000000;
	mov.f64 	%fd997, 0d0000000000000000;
	rcp.rn.f64 	%fd998, %fd997;
	selp.f64 	%fd1488, 0d0000000000000000, %fd998, %p113;
	bra.uni 	$L__BB0_133;

$L__BB0_11:
	setp.eq.f64 	%p14, %fd1428, 0dBFF0000000000000;
	@%p14 bra 	$L__BB0_33;

	setp.gt.f64 	%p15, %fd15, 0d3FF0000000000000;
	mov.f64 	%fd221, 0d0000000000000000;
	rcp.rn.f64 	%fd222, %fd221;
	selp.f64 	%fd1437, 0d0000000000000000, %fd222, %p15;
	bra.uni 	$L__BB0_33;

$L__BB0_144:
	setp.eq.f64 	%p143, %fd164, 0dBFF0000000000000;
	@%p143 bra 	$L__BB0_166;

	setp.gt.f64 	%p144, %fd165, 0d3FF0000000000000;
	mov.f64 	%fd1198, 0d0000000000000000;
	rcp.rn.f64 	%fd1199, %fd1198;
	selp.f64 	%fd1494, 0d0000000000000000, %fd1199, %p144;
	bra.uni 	$L__BB0_166;

$L__BB0_43:
	mov.f64 	%fd422, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r126, %temp}, %fd422;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r127}, %fd422;
	}
	and.b32  	%r128, %r127, 2147483647;
	setp.ne.s32 	%p44, %r128, 2146435072;
	setp.ne.s32 	%p45, %r126, 0;
	or.pred  	%p46, %p45, %p44;
	@%p46 bra 	$L__BB0_46;
	bra.uni 	$L__BB0_44;

$L__BB0_46:
	mov.f64 	%fd427, 0d3FE0000000000000;
	mul.rn.f64 	%fd428, %fd427, %fd422;
	cvt.rzi.f64.f64 	%fd429, %fd428;
	mov.f64 	%fd430, 0d4000000000000000;
	mul.rn.f64 	%fd431, %fd430, %fd429;
	sub.f64 	%fd432, %fd422, %fd431;
	abs.f64 	%fd48, %fd432;
	setp.eq.f64 	%p49, %fd1440, 0dFFF0000000000000;
	@%p49 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_47;

$L__BB0_60:
	setp.neu.f64 	%p63, %fd48, 0d3FF0000000000000;
	mov.f64 	%fd1447, 0d0000000000000000;
	@%p63 bra 	$L__BB0_64;

	mov.f64 	%fd1447, 0d8000000000000000;
	bra.uni 	$L__BB0_64;

$L__BB0_114:
	setp.eq.f64 	%p115, %fd1484, 0dFFF0000000000000;
	@%p115 bra 	$L__BB0_128;
	bra.uni 	$L__BB0_115;

$L__BB0_128:
	setp.neu.f64 	%p129, %fd140, 0d3FF0000000000000;
	mov.f64 	%fd1488, 0d0000000000000000;
	@%p129 bra 	$L__BB0_133;

	mov.f64 	%fd1488, 0d8000000000000000;
	bra.uni 	$L__BB0_133;

$L__BB0_14:
	setp.eq.f64 	%p17, %fd1428, 0dFFF0000000000000;
	@%p17 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_15;

$L__BB0_28:
	setp.neu.f64 	%p31, %fd17, 0d3FF0000000000000;
	mov.f64 	%fd1437, 0d0000000000000000;
	@%p31 bra 	$L__BB0_33;

	mov.f64 	%fd1437, 0d8000000000000000;
	bra.uni 	$L__BB0_33;

$L__BB0_147:
	setp.eq.f64 	%p146, %fd164, 0dFFF0000000000000;
	@%p146 bra 	$L__BB0_161;
	bra.uni 	$L__BB0_148;

$L__BB0_161:
	setp.neu.f64 	%p160, %fd167, 0d3FF0000000000000;
	mov.f64 	%fd1494, 0d0000000000000000;
	@%p160 bra 	$L__BB0_166;

	mov.f64 	%fd1494, 0d8000000000000000;
	bra.uni 	$L__BB0_166;

$L__BB0_44:
	setp.eq.f64 	%p47, %fd1440, 0dBFF0000000000000;
	@%p47 bra 	$L__BB0_64;

	setp.gt.f64 	%p48, %fd46, 0d3FF0000000000000;
	mov.f64 	%fd424, 0d0000000000000000;
	rcp.rn.f64 	%fd425, %fd424;
	selp.f64 	%fd1447, 0d0000000000000000, %fd425, %p48;
	bra.uni 	$L__BB0_64;

$L__BB0_115:
	setp.geu.f64 	%p116, %fd1484, 0d0000000000000000;
	@%p116 bra 	$L__BB0_117;

	mov.f64 	%fd1007, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd1008, %fd1007;
	setp.neu.f64 	%p117, %fd1008, 0dBFF0000000000000;
	mov.f64 	%fd1488, 0dFFF8000000000000;
	@%p117 bra 	$L__BB0_133;

$L__BB0_117:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r290}, %fd138; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r289, hi}, %fd138; 
	}
	// end inline asm
	bfe.u32 	%r291, %r290, 20, 11;
	setp.ne.s32 	%p118, %r291, 0;
	@%p118 bra 	$L__BB0_119;

	mov.f64 	%fd1013, 0d4350000000000000;
	mul.rn.f64 	%fd1012, %fd138, %fd1013;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r290}, %fd1012; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r289, hi}, %fd1012; 
	}
	// end inline asm
	bfe.u32 	%r202, %r290, 20, 11;
	add.s32 	%r291, %r202, -54;

$L__BB0_119:
	add.s32 	%r292, %r291, -1023;
	and.b32  	%r205, %r290, -2146435073;
	or.b32  	%r204, %r205, 1072693248;
	// begin inline asm
	mov.b64 	%fd1485, {%r289, %r204};
	// end inline asm
	setp.lt.u32 	%p119, %r204, 1073127583;
	@%p119 bra 	$L__BB0_121;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r206, hi}, %fd1485; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r207}, %fd1485; 
	}
	// end inline asm
	add.s32 	%r209, %r207, -1048576;
	// begin inline asm
	mov.b64 	%fd1485, {%r206, %r209};
	// end inline asm
	add.s32 	%r292, %r291, -1022;

$L__BB0_121:
	add.f64 	%fd1102, %fd1485, 0d3FF0000000000000;
	mov.f64 	%fd1103, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1104, %fd1102;
	add.f64 	%fd1044, %fd1485, 0dBFF0000000000000;
	mov.f64 	%fd1100, 0dBFF0000000000000;
	mul.rn.f64 	%fd1105, %fd1044, %fd1104;
	add.f64 	%fd1092, %fd1105, %fd1105;
	mul.rn.f64 	%fd1040, %fd1092, %fd1092;
	mov.f64 	%fd1019, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1021, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1018, %fd1019, %fd1040, %fd1021;
	// end inline asm
	mov.f64 	%fd1025, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1022, %fd1018, %fd1040, %fd1025;
	// end inline asm
	mov.f64 	%fd1029, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1026, %fd1022, %fd1040, %fd1029;
	// end inline asm
	mov.f64 	%fd1033, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1030, %fd1026, %fd1040, %fd1033;
	// end inline asm
	mov.f64 	%fd1037, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1034, %fd1030, %fd1040, %fd1037;
	// end inline asm
	mov.f64 	%fd1041, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1038, %fd1034, %fd1040, %fd1041;
	// end inline asm
	mul.rn.f64 	%fd1106, %fd1038, %fd1040;
	sub.f64 	%fd1107, %fd1044, %fd1092;
	mov.f64 	%fd1108, 0d4000000000000000;
	mul.rn.f64 	%fd1045, %fd1108, %fd1107;
	neg.f64 	%fd1043, %fd1092;
	// begin inline asm
	fma.rn.f64 	%fd1042, %fd1043, %fd1044, %fd1045;
	// end inline asm
	mul.rn.f64 	%fd1088, %fd1104, %fd1042;
	add.f64 	%fd1109, %fd1106, 0d3FB5555555555555;
	mov.f64 	%fd1110, 0d3FB5555555555555;
	sub.f64 	%fd1111, %fd1110, %fd1109;
	add.f64 	%fd1112, %fd1106, %fd1111;
	add.f64 	%fd1113, %fd1112, 0d0000000000000000;
	add.f64 	%fd1114, %fd1113, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1055, %fd1109, %fd1114;
	sub.f64 	%fd1115, %fd1109, %fd1055;
	add.f64 	%fd1059, %fd1114, %fd1115;
	mul.rn.f64 	%fd1116, %fd1055, %fd1092;
	neg.f64 	%fd1049, %fd1116;
	// begin inline asm
	fma.rn.f64 	%fd1046, %fd1055, %fd1092, %fd1049;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1050, %fd1059, %fd1088, %fd1046;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1054, %fd1055, %fd1088, %fd1050;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1058, %fd1059, %fd1092, %fd1054;
	// end inline asm
	add.f64 	%fd1071, %fd1116, %fd1058;
	sub.f64 	%fd1117, %fd1116, %fd1071;
	add.f64 	%fd1075, %fd1058, %fd1117;
	mul.rn.f64 	%fd1118, %fd1071, %fd1092;
	neg.f64 	%fd1065, %fd1118;
	// begin inline asm
	fma.rn.f64 	%fd1062, %fd1071, %fd1092, %fd1065;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1066, %fd1075, %fd1088, %fd1062;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1070, %fd1071, %fd1088, %fd1066;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1074, %fd1075, %fd1092, %fd1070;
	// end inline asm
	add.f64 	%fd1087, %fd1118, %fd1074;
	sub.f64 	%fd1119, %fd1118, %fd1087;
	add.f64 	%fd1091, %fd1074, %fd1119;
	mul.rn.f64 	%fd1120, %fd1087, %fd1092;
	neg.f64 	%fd1081, %fd1120;
	// begin inline asm
	fma.rn.f64 	%fd1078, %fd1087, %fd1092, %fd1081;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1082, %fd1091, %fd1088, %fd1078;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1086, %fd1087, %fd1088, %fd1082;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1090, %fd1091, %fd1092, %fd1086;
	// end inline asm
	add.f64 	%fd1121, %fd1120, %fd1090;
	sub.f64 	%fd1122, %fd1120, %fd1121;
	add.f64 	%fd1123, %fd1090, %fd1122;
	add.f64 	%fd1124, %fd1092, %fd1121;
	sub.f64 	%fd1125, %fd1092, %fd1124;
	add.f64 	%fd1126, %fd1121, %fd1125;
	add.f64 	%fd1127, %fd1123, %fd1126;
	add.f64 	%fd1128, %fd1088, %fd1127;
	add.f64 	%fd1129, %fd1124, %fd1128;
	sub.f64 	%fd1130, %fd1124, %fd1129;
	add.f64 	%fd1131, %fd1128, %fd1130;
	cvt.rn.f64.s32 	%fd1132, %r292;
	mov.f64 	%fd1133, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1134, %fd1132, %fd1133;
	mov.f64 	%fd1135, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1136, %fd1132, %fd1135;
	add.f64 	%fd1137, %fd1134, %fd1129;
	sub.f64 	%fd1138, %fd1134, %fd1137;
	add.f64 	%fd1139, %fd1129, %fd1138;
	add.f64 	%fd1140, %fd1131, %fd1139;
	add.f64 	%fd1141, %fd1136, %fd1140;
	add.f64 	%fd1095, %fd1137, %fd1141;
	sub.f64 	%fd1142, %fd1137, %fd1095;
	add.f64 	%fd1099, %fd1141, %fd1142;
	mul.rn.f64 	%fd1143, %fd1095, %fd1100;
	neg.f64 	%fd1097, %fd1143;
	// begin inline asm
	fma.rn.f64 	%fd1094, %fd1095, %fd1100, %fd1097;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1098, %fd1099, %fd1100, %fd1094;
	// end inline asm
	add.f64 	%fd144, %fd1143, %fd1098;
	sub.f64 	%fd1144, %fd1143, %fd144;
	add.f64 	%fd145, %fd1098, %fd1144;
	mov.f64 	%fd1145, 0d4338000000000000;
	mov.f64 	%fd1146, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1147, %fd144, %fd1146, %fd1145;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r68, %temp}, %fd1147;
	}
	mov.f64 	%fd1148, 0dC338000000000000;
	add.rn.f64 	%fd1149, %fd1147, %fd1148;
	mov.f64 	%fd1150, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1151, %fd1149, %fd1150, %fd144;
	mov.f64 	%fd1152, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1153, %fd1149, %fd1152, %fd1151;
	mov.f64 	%fd1154, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1155, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1156, %fd1155, %fd1153, %fd1154;
	mov.f64 	%fd1157, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1158, %fd1156, %fd1153, %fd1157;
	mov.f64 	%fd1159, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1160, %fd1158, %fd1153, %fd1159;
	mov.f64 	%fd1161, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1162, %fd1160, %fd1153, %fd1161;
	mov.f64 	%fd1163, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1164, %fd1162, %fd1153, %fd1163;
	mov.f64 	%fd1165, 0d3F81111111122322;
	fma.rn.f64 	%fd1166, %fd1164, %fd1153, %fd1165;
	mov.f64 	%fd1167, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1168, %fd1166, %fd1153, %fd1167;
	mov.f64 	%fd1169, 0d3FC5555555555511;
	fma.rn.f64 	%fd1170, %fd1168, %fd1153, %fd1169;
	mov.f64 	%fd1171, 0d3FE000000000000B;
	fma.rn.f64 	%fd1172, %fd1170, %fd1153, %fd1171;
	fma.rn.f64 	%fd1173, %fd1172, %fd1153, %fd1103;
	fma.rn.f64 	%fd1174, %fd1173, %fd1153, %fd1103;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r69, %temp}, %fd1174;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd1174;
	}
	shl.b32 	%r210, %r68, 20;
	add.s32 	%r211, %r70, %r210;
	mov.b64 	%fd1488, {%r69, %r211};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r212}, %fd144;
	}
	mov.b32 	%f11, %r212;
	abs.f32 	%f3, %f11;
	setp.lt.f32 	%p120, %f3, 0f4086232B;
	@%p120 bra 	$L__BB0_124;

	setp.lt.f64 	%p121, %fd144, 0d0000000000000000;
	add.f64 	%fd1175, %fd144, 0d7FF0000000000000;
	selp.f64 	%fd1488, 0d0000000000000000, %fd1175, %p121;
	setp.geu.f32 	%p122, %f3, 0f40874800;
	@%p122 bra 	$L__BB0_124;

	shr.u32 	%r213, %r68, 31;
	add.s32 	%r214, %r68, %r213;
	shr.s32 	%r215, %r214, 1;
	shl.b32 	%r216, %r215, 20;
	add.s32 	%r217, %r70, %r216;
	mov.b64 	%fd1176, {%r69, %r217};
	sub.s32 	%r218, %r68, %r215;
	shl.b32 	%r219, %r218, 20;
	add.s32 	%r220, %r219, 1072693248;
	mov.u32 	%r221, 0;
	mov.b64 	%fd1177, {%r221, %r220};
	mul.f64 	%fd1488, %fd1176, %fd1177;

$L__BB0_124:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r222}, %fd1488;
	}
	and.b32  	%r223, %r222, 2147483647;
	setp.eq.s32 	%p123, %r223, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r224, %temp}, %fd1488;
	}
	setp.eq.s32 	%p124, %r224, 0;
	and.pred  	%p125, %p124, %p123;
	@%p125 bra 	$L__BB0_126;

	// begin inline asm
	fma.rn.f64 	%fd1488, %fd1488, %fd145, %fd1488;
	// end inline asm

$L__BB0_126:
	setp.neu.f64 	%p126, %fd140, 0d3FF0000000000000;
	or.pred  	%p128, %p116, %p126;
	@%p128 bra 	$L__BB0_133;

	mov.b64 	%rd44, %fd1488;
	xor.b64  	%rd45, %rd44, -9223372036854775808;
	mov.b64 	%fd1488, %rd45;
	bra.uni 	$L__BB0_133;

$L__BB0_47:
	setp.geu.f64 	%p50, %fd1440, 0d0000000000000000;
	@%p50 bra 	$L__BB0_49;

	mov.f64 	%fd434, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd435, %fd434;
	setp.neu.f64 	%p51, %fd435, 0dBFF0000000000000;
	mov.f64 	%fd1447, 0dFFF8000000000000;
	@%p51 bra 	$L__BB0_64;

$L__BB0_49:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r275}, %fd46; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r274, hi}, %fd46; 
	}
	// end inline asm
	bfe.u32 	%r276, %r275, 20, 11;
	setp.ne.s32 	%p52, %r276, 0;
	@%p52 bra 	$L__BB0_51;

	mov.f64 	%fd440, 0d4350000000000000;
	mul.rn.f64 	%fd439, %fd46, %fd440;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r275}, %fd439; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r274, hi}, %fd439; 
	}
	// end inline asm
	bfe.u32 	%r133, %r275, 20, 11;
	add.s32 	%r276, %r133, -54;

$L__BB0_51:
	and.b32  	%r136, %r275, -2146435073;
	or.b32  	%r135, %r136, 1072693248;
	// begin inline asm
	mov.b64 	%fd1444, {%r274, %r135};
	// end inline asm
	setp.lt.u32 	%p53, %r135, 1073127583;
	add.s32 	%r277, %r276, -1023;
	@%p53 bra 	$L__BB0_53;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r137, hi}, %fd1444; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r138}, %fd1444; 
	}
	// end inline asm
	add.s32 	%r140, %r138, -1048576;
	// begin inline asm
	mov.b64 	%fd1444, {%r137, %r140};
	// end inline asm
	add.s32 	%r277, %r276, -1022;

$L__BB0_53:
	add.f64 	%fd529, %fd1444, 0d3FF0000000000000;
	mov.f64 	%fd530, 0d3FF0000000000000;
	rcp.rn.f64 	%fd531, %fd529;
	add.f64 	%fd471, %fd1444, 0dBFF0000000000000;
	mov.f64 	%fd527, 0dBFF0000000000000;
	mul.rn.f64 	%fd532, %fd471, %fd531;
	add.f64 	%fd519, %fd532, %fd532;
	mul.rn.f64 	%fd467, %fd519, %fd519;
	mov.f64 	%fd446, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd448, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd445, %fd446, %fd467, %fd448;
	// end inline asm
	mov.f64 	%fd452, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd449, %fd445, %fd467, %fd452;
	// end inline asm
	mov.f64 	%fd456, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd453, %fd449, %fd467, %fd456;
	// end inline asm
	mov.f64 	%fd460, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd457, %fd453, %fd467, %fd460;
	// end inline asm
	mov.f64 	%fd464, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd461, %fd457, %fd467, %fd464;
	// end inline asm
	mov.f64 	%fd468, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd465, %fd461, %fd467, %fd468;
	// end inline asm
	mul.rn.f64 	%fd533, %fd465, %fd467;
	sub.f64 	%fd534, %fd471, %fd519;
	mov.f64 	%fd535, 0d4000000000000000;
	mul.rn.f64 	%fd472, %fd535, %fd534;
	neg.f64 	%fd470, %fd519;
	// begin inline asm
	fma.rn.f64 	%fd469, %fd470, %fd471, %fd472;
	// end inline asm
	mul.rn.f64 	%fd515, %fd531, %fd469;
	add.f64 	%fd536, %fd533, 0d3FB5555555555555;
	mov.f64 	%fd537, 0d3FB5555555555555;
	sub.f64 	%fd538, %fd537, %fd536;
	add.f64 	%fd539, %fd533, %fd538;
	add.f64 	%fd540, %fd539, 0d0000000000000000;
	add.f64 	%fd541, %fd540, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd482, %fd536, %fd541;
	sub.f64 	%fd542, %fd536, %fd482;
	add.f64 	%fd486, %fd541, %fd542;
	mul.rn.f64 	%fd543, %fd482, %fd519;
	neg.f64 	%fd476, %fd543;
	// begin inline asm
	fma.rn.f64 	%fd473, %fd482, %fd519, %fd476;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd477, %fd486, %fd515, %fd473;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd481, %fd482, %fd515, %fd477;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd485, %fd486, %fd519, %fd481;
	// end inline asm
	add.f64 	%fd498, %fd543, %fd485;
	sub.f64 	%fd544, %fd543, %fd498;
	add.f64 	%fd502, %fd485, %fd544;
	mul.rn.f64 	%fd545, %fd498, %fd519;
	neg.f64 	%fd492, %fd545;
	// begin inline asm
	fma.rn.f64 	%fd489, %fd498, %fd519, %fd492;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd493, %fd502, %fd515, %fd489;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd497, %fd498, %fd515, %fd493;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd501, %fd502, %fd519, %fd497;
	// end inline asm
	add.f64 	%fd514, %fd545, %fd501;
	sub.f64 	%fd546, %fd545, %fd514;
	add.f64 	%fd518, %fd501, %fd546;
	mul.rn.f64 	%fd547, %fd514, %fd519;
	neg.f64 	%fd508, %fd547;
	// begin inline asm
	fma.rn.f64 	%fd505, %fd514, %fd519, %fd508;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd509, %fd518, %fd515, %fd505;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd513, %fd514, %fd515, %fd509;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd517, %fd518, %fd519, %fd513;
	// end inline asm
	add.f64 	%fd548, %fd547, %fd517;
	sub.f64 	%fd549, %fd547, %fd548;
	add.f64 	%fd550, %fd517, %fd549;
	add.f64 	%fd551, %fd519, %fd548;
	sub.f64 	%fd552, %fd519, %fd551;
	add.f64 	%fd553, %fd548, %fd552;
	add.f64 	%fd554, %fd550, %fd553;
	add.f64 	%fd555, %fd515, %fd554;
	add.f64 	%fd556, %fd551, %fd555;
	sub.f64 	%fd557, %fd551, %fd556;
	add.f64 	%fd558, %fd555, %fd557;
	cvt.rn.f64.s32 	%fd559, %r277;
	mov.f64 	%fd560, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd561, %fd559, %fd560;
	mov.f64 	%fd562, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd563, %fd559, %fd562;
	add.f64 	%fd564, %fd561, %fd556;
	sub.f64 	%fd565, %fd561, %fd564;
	add.f64 	%fd566, %fd556, %fd565;
	add.f64 	%fd567, %fd558, %fd566;
	add.f64 	%fd568, %fd563, %fd567;
	add.f64 	%fd522, %fd564, %fd568;
	sub.f64 	%fd569, %fd564, %fd522;
	add.f64 	%fd526, %fd568, %fd569;
	mul.rn.f64 	%fd570, %fd522, %fd527;
	neg.f64 	%fd524, %fd570;
	// begin inline asm
	fma.rn.f64 	%fd521, %fd522, %fd527, %fd524;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd525, %fd526, %fd527, %fd521;
	// end inline asm
	add.f64 	%fd52, %fd570, %fd525;
	sub.f64 	%fd571, %fd570, %fd52;
	add.f64 	%fd53, %fd525, %fd571;
	mov.f64 	%fd572, 0d4338000000000000;
	mov.f64 	%fd573, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd574, %fd52, %fd573, %fd572;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r35, %temp}, %fd574;
	}
	mov.f64 	%fd575, 0dC338000000000000;
	add.rn.f64 	%fd576, %fd574, %fd575;
	mov.f64 	%fd577, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd578, %fd576, %fd577, %fd52;
	mov.f64 	%fd579, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd580, %fd576, %fd579, %fd578;
	mov.f64 	%fd581, 0d3E928AF3FCA213EA;
	mov.f64 	%fd582, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd583, %fd582, %fd580, %fd581;
	mov.f64 	%fd584, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd585, %fd583, %fd580, %fd584;
	mov.f64 	%fd586, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd587, %fd585, %fd580, %fd586;
	mov.f64 	%fd588, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd589, %fd587, %fd580, %fd588;
	mov.f64 	%fd590, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd591, %fd589, %fd580, %fd590;
	mov.f64 	%fd592, 0d3F81111111122322;
	fma.rn.f64 	%fd593, %fd591, %fd580, %fd592;
	mov.f64 	%fd594, 0d3FA55555555502A1;
	fma.rn.f64 	%fd595, %fd593, %fd580, %fd594;
	mov.f64 	%fd596, 0d3FC5555555555511;
	fma.rn.f64 	%fd597, %fd595, %fd580, %fd596;
	mov.f64 	%fd598, 0d3FE000000000000B;
	fma.rn.f64 	%fd599, %fd597, %fd580, %fd598;
	fma.rn.f64 	%fd600, %fd599, %fd580, %fd530;
	fma.rn.f64 	%fd601, %fd600, %fd580, %fd530;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd601;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd601;
	}
	shl.b32 	%r141, %r35, 20;
	add.s32 	%r142, %r37, %r141;
	mov.b64 	%fd1447, {%r36, %r142};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r143}, %fd52;
	}
	mov.b32 	%f6, %r143;
	abs.f32 	%f2, %f6;
	setp.lt.f32 	%p54, %f2, 0f4086232B;
	@%p54 bra 	$L__BB0_56;

	setp.lt.f64 	%p55, %fd52, 0d0000000000000000;
	add.f64 	%fd602, %fd52, 0d7FF0000000000000;
	selp.f64 	%fd1447, 0d0000000000000000, %fd602, %p55;
	setp.geu.f32 	%p56, %f2, 0f40874800;
	@%p56 bra 	$L__BB0_56;

	shr.u32 	%r144, %r35, 31;
	add.s32 	%r145, %r35, %r144;
	shr.s32 	%r146, %r145, 1;
	shl.b32 	%r147, %r146, 20;
	add.s32 	%r148, %r37, %r147;
	mov.b64 	%fd603, {%r36, %r148};
	sub.s32 	%r149, %r35, %r146;
	shl.b32 	%r150, %r149, 20;
	add.s32 	%r151, %r150, 1072693248;
	mov.u32 	%r152, 0;
	mov.b64 	%fd604, {%r152, %r151};
	mul.f64 	%fd1447, %fd603, %fd604;

$L__BB0_56:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r153}, %fd1447;
	}
	and.b32  	%r154, %r153, 2147483647;
	setp.eq.s32 	%p57, %r154, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r155, %temp}, %fd1447;
	}
	setp.eq.s32 	%p58, %r155, 0;
	and.pred  	%p59, %p58, %p57;
	@%p59 bra 	$L__BB0_58;

	// begin inline asm
	fma.rn.f64 	%fd1447, %fd1447, %fd53, %fd1447;
	// end inline asm

$L__BB0_58:
	setp.neu.f64 	%p60, %fd48, 0d3FF0000000000000;
	or.pred  	%p62, %p50, %p60;
	@%p62 bra 	$L__BB0_64;

	mov.b64 	%rd24, %fd1447;
	xor.b64  	%rd25, %rd24, -9223372036854775808;
	mov.b64 	%fd1447, %rd25;
	bra.uni 	$L__BB0_64;

$L__BB0_15:
	setp.geu.f64 	%p18, %fd1428, 0d0000000000000000;
	@%p18 bra 	$L__BB0_17;

	mov.f64 	%fd231, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd232, %fd231;
	setp.neu.f64 	%p19, %fd232, 0dBFF0000000000000;
	mov.f64 	%fd1437, 0dFFF8000000000000;
	@%p19 bra 	$L__BB0_33;

$L__BB0_17:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r269}, %fd15; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r268, hi}, %fd15; 
	}
	// end inline asm
	bfe.u32 	%r270, %r269, 20, 11;
	setp.ne.s32 	%p20, %r270, 0;
	@%p20 bra 	$L__BB0_19;

	mov.f64 	%fd237, 0d4350000000000000;
	mul.rn.f64 	%fd236, %fd15, %fd237;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r269}, %fd236; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r268, hi}, %fd236; 
	}
	// end inline asm
	bfe.u32 	%r100, %r269, 20, 11;
	add.s32 	%r270, %r100, -54;

$L__BB0_19:
	and.b32  	%r103, %r269, -2146435073;
	or.b32  	%r102, %r103, 1072693248;
	// begin inline asm
	mov.b64 	%fd1434, {%r268, %r102};
	// end inline asm
	setp.lt.u32 	%p21, %r102, 1073127583;
	add.s32 	%r271, %r270, -1023;
	@%p21 bra 	$L__BB0_21;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r104, hi}, %fd1434; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r105}, %fd1434; 
	}
	// end inline asm
	add.s32 	%r107, %r105, -1048576;
	// begin inline asm
	mov.b64 	%fd1434, {%r104, %r107};
	// end inline asm
	add.s32 	%r271, %r270, -1022;

$L__BB0_21:
	add.f64 	%fd326, %fd1434, 0d3FF0000000000000;
	mov.f64 	%fd327, 0d3FF0000000000000;
	rcp.rn.f64 	%fd328, %fd326;
	add.f64 	%fd268, %fd1434, 0dBFF0000000000000;
	mov.f64 	%fd324, 0dBFF0000000000000;
	mul.rn.f64 	%fd329, %fd268, %fd328;
	add.f64 	%fd316, %fd329, %fd329;
	mul.rn.f64 	%fd264, %fd316, %fd316;
	mov.f64 	%fd243, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd245, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd242, %fd243, %fd264, %fd245;
	// end inline asm
	mov.f64 	%fd249, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd246, %fd242, %fd264, %fd249;
	// end inline asm
	mov.f64 	%fd253, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd250, %fd246, %fd264, %fd253;
	// end inline asm
	mov.f64 	%fd257, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd254, %fd250, %fd264, %fd257;
	// end inline asm
	mov.f64 	%fd261, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd258, %fd254, %fd264, %fd261;
	// end inline asm
	mov.f64 	%fd265, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd262, %fd258, %fd264, %fd265;
	// end inline asm
	mul.rn.f64 	%fd330, %fd262, %fd264;
	sub.f64 	%fd331, %fd268, %fd316;
	mov.f64 	%fd332, 0d4000000000000000;
	mul.rn.f64 	%fd269, %fd332, %fd331;
	neg.f64 	%fd267, %fd316;
	// begin inline asm
	fma.rn.f64 	%fd266, %fd267, %fd268, %fd269;
	// end inline asm
	mul.rn.f64 	%fd312, %fd328, %fd266;
	add.f64 	%fd333, %fd330, 0d3FB5555555555555;
	mov.f64 	%fd334, 0d3FB5555555555555;
	sub.f64 	%fd335, %fd334, %fd333;
	add.f64 	%fd336, %fd330, %fd335;
	add.f64 	%fd337, %fd336, 0d0000000000000000;
	add.f64 	%fd338, %fd337, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd279, %fd333, %fd338;
	sub.f64 	%fd339, %fd333, %fd279;
	add.f64 	%fd283, %fd338, %fd339;
	mul.rn.f64 	%fd340, %fd279, %fd316;
	neg.f64 	%fd273, %fd340;
	// begin inline asm
	fma.rn.f64 	%fd270, %fd279, %fd316, %fd273;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd274, %fd283, %fd312, %fd270;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd278, %fd279, %fd312, %fd274;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd282, %fd283, %fd316, %fd278;
	// end inline asm
	add.f64 	%fd295, %fd340, %fd282;
	sub.f64 	%fd341, %fd340, %fd295;
	add.f64 	%fd299, %fd282, %fd341;
	mul.rn.f64 	%fd342, %fd295, %fd316;
	neg.f64 	%fd289, %fd342;
	// begin inline asm
	fma.rn.f64 	%fd286, %fd295, %fd316, %fd289;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd290, %fd299, %fd312, %fd286;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd294, %fd295, %fd312, %fd290;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd298, %fd299, %fd316, %fd294;
	// end inline asm
	add.f64 	%fd311, %fd342, %fd298;
	sub.f64 	%fd343, %fd342, %fd311;
	add.f64 	%fd315, %fd298, %fd343;
	mul.rn.f64 	%fd344, %fd311, %fd316;
	neg.f64 	%fd305, %fd344;
	// begin inline asm
	fma.rn.f64 	%fd302, %fd311, %fd316, %fd305;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd306, %fd315, %fd312, %fd302;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd310, %fd311, %fd312, %fd306;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd314, %fd315, %fd316, %fd310;
	// end inline asm
	add.f64 	%fd345, %fd344, %fd314;
	sub.f64 	%fd346, %fd344, %fd345;
	add.f64 	%fd347, %fd314, %fd346;
	add.f64 	%fd348, %fd316, %fd345;
	sub.f64 	%fd349, %fd316, %fd348;
	add.f64 	%fd350, %fd345, %fd349;
	add.f64 	%fd351, %fd347, %fd350;
	add.f64 	%fd352, %fd312, %fd351;
	add.f64 	%fd353, %fd348, %fd352;
	sub.f64 	%fd354, %fd348, %fd353;
	add.f64 	%fd355, %fd352, %fd354;
	cvt.rn.f64.s32 	%fd356, %r271;
	mov.f64 	%fd357, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd358, %fd356, %fd357;
	mov.f64 	%fd359, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd360, %fd356, %fd359;
	add.f64 	%fd361, %fd358, %fd353;
	sub.f64 	%fd362, %fd358, %fd361;
	add.f64 	%fd363, %fd353, %fd362;
	add.f64 	%fd364, %fd355, %fd363;
	add.f64 	%fd365, %fd360, %fd364;
	add.f64 	%fd319, %fd361, %fd365;
	sub.f64 	%fd366, %fd361, %fd319;
	add.f64 	%fd323, %fd365, %fd366;
	mul.rn.f64 	%fd367, %fd319, %fd324;
	neg.f64 	%fd321, %fd367;
	// begin inline asm
	fma.rn.f64 	%fd318, %fd319, %fd324, %fd321;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd322, %fd323, %fd324, %fd318;
	// end inline asm
	add.f64 	%fd21, %fd367, %fd322;
	sub.f64 	%fd368, %fd367, %fd21;
	add.f64 	%fd22, %fd322, %fd368;
	mov.f64 	%fd369, 0d4338000000000000;
	mov.f64 	%fd370, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd371, %fd21, %fd370, %fd369;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd371;
	}
	mov.f64 	%fd372, 0dC338000000000000;
	add.rn.f64 	%fd373, %fd371, %fd372;
	mov.f64 	%fd374, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd375, %fd373, %fd374, %fd21;
	mov.f64 	%fd376, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd377, %fd373, %fd376, %fd375;
	mov.f64 	%fd378, 0d3E928AF3FCA213EA;
	mov.f64 	%fd379, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd380, %fd379, %fd377, %fd378;
	mov.f64 	%fd381, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd382, %fd380, %fd377, %fd381;
	mov.f64 	%fd383, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd384, %fd382, %fd377, %fd383;
	mov.f64 	%fd385, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd386, %fd384, %fd377, %fd385;
	mov.f64 	%fd387, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd388, %fd386, %fd377, %fd387;
	mov.f64 	%fd389, 0d3F81111111122322;
	fma.rn.f64 	%fd390, %fd388, %fd377, %fd389;
	mov.f64 	%fd391, 0d3FA55555555502A1;
	fma.rn.f64 	%fd392, %fd390, %fd377, %fd391;
	mov.f64 	%fd393, 0d3FC5555555555511;
	fma.rn.f64 	%fd394, %fd392, %fd377, %fd393;
	mov.f64 	%fd395, 0d3FE000000000000B;
	fma.rn.f64 	%fd396, %fd394, %fd377, %fd395;
	fma.rn.f64 	%fd397, %fd396, %fd377, %fd327;
	fma.rn.f64 	%fd398, %fd397, %fd377, %fd327;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd398;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd398;
	}
	shl.b32 	%r108, %r17, 20;
	add.s32 	%r109, %r19, %r108;
	mov.b64 	%fd1437, {%r18, %r109};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r110}, %fd21;
	}
	mov.b32 	%f5, %r110;
	abs.f32 	%f1, %f5;
	setp.lt.f32 	%p22, %f1, 0f4086232B;
	@%p22 bra 	$L__BB0_24;

	setp.lt.f64 	%p23, %fd21, 0d0000000000000000;
	add.f64 	%fd399, %fd21, 0d7FF0000000000000;
	selp.f64 	%fd1437, 0d0000000000000000, %fd399, %p23;
	setp.geu.f32 	%p24, %f1, 0f40874800;
	@%p24 bra 	$L__BB0_24;

	mov.f64 	%fd1424, 0d4338000000000000;
	mov.f64 	%fd1423, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1422, %fd21, %fd1423, %fd1424;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r265, %temp}, %fd1422;
	}
	shr.u32 	%r111, %r265, 31;
	add.s32 	%r112, %r265, %r111;
	shr.s32 	%r113, %r112, 1;
	shl.b32 	%r114, %r113, 20;
	add.s32 	%r115, %r19, %r114;
	mov.b64 	%fd400, {%r18, %r115};
	sub.s32 	%r116, %r265, %r113;
	shl.b32 	%r117, %r116, 20;
	add.s32 	%r118, %r117, 1072693248;
	mov.u32 	%r119, 0;
	mov.b64 	%fd401, {%r119, %r118};
	mul.f64 	%fd1437, %fd400, %fd401;

$L__BB0_24:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r120}, %fd1437;
	}
	and.b32  	%r121, %r120, 2147483647;
	setp.eq.s32 	%p25, %r121, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r122, %temp}, %fd1437;
	}
	setp.eq.s32 	%p26, %r122, 0;
	and.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB0_26;

	// begin inline asm
	fma.rn.f64 	%fd1437, %fd1437, %fd22, %fd1437;
	// end inline asm

$L__BB0_26:
	setp.neu.f64 	%p28, %fd17, 0d3FF0000000000000;
	or.pred  	%p30, %p18, %p28;
	@%p30 bra 	$L__BB0_33;

	mov.b64 	%rd19, %fd1437;
	xor.b64  	%rd20, %rd19, -9223372036854775808;
	mov.b64 	%fd1437, %rd20;
	bra.uni 	$L__BB0_33;

$L__BB0_148:
	setp.geu.f64 	%p147, %fd164, 0d0000000000000000;
	@%p147 bra 	$L__BB0_150;

	mov.f64 	%fd1208, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd1209, %fd1208;
	setp.neu.f64 	%p148, %fd1209, 0dBFF0000000000000;
	mov.f64 	%fd1494, 0dFFF8000000000000;
	@%p148 bra 	$L__BB0_166;

$L__BB0_150:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r294}, %fd165; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r293, hi}, %fd165; 
	}
	// end inline asm
	bfe.u32 	%r295, %r294, 20, 11;
	setp.ne.s32 	%p149, %r295, 0;
	@%p149 bra 	$L__BB0_152;

	mov.f64 	%fd1214, 0d4350000000000000;
	mul.rn.f64 	%fd1213, %fd165, %fd1214;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r294}, %fd1213; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r293, hi}, %fd1213; 
	}
	// end inline asm
	bfe.u32 	%r233, %r294, 20, 11;
	add.s32 	%r295, %r233, -54;

$L__BB0_152:
	add.s32 	%r296, %r295, -1023;
	and.b32  	%r236, %r294, -2146435073;
	or.b32  	%r235, %r236, 1072693248;
	// begin inline asm
	mov.b64 	%fd1491, {%r293, %r235};
	// end inline asm
	setp.lt.u32 	%p150, %r235, 1073127583;
	@%p150 bra 	$L__BB0_154;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r237, hi}, %fd1491; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r238}, %fd1491; 
	}
	// end inline asm
	add.s32 	%r240, %r238, -1048576;
	// begin inline asm
	mov.b64 	%fd1491, {%r237, %r240};
	// end inline asm
	add.s32 	%r296, %r295, -1022;

$L__BB0_154:
	add.f64 	%fd1303, %fd1491, 0d3FF0000000000000;
	mov.f64 	%fd1304, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1305, %fd1303;
	add.f64 	%fd1245, %fd1491, 0dBFF0000000000000;
	mov.f64 	%fd1301, 0dBFF0000000000000;
	mul.rn.f64 	%fd1306, %fd1245, %fd1305;
	add.f64 	%fd1293, %fd1306, %fd1306;
	mul.rn.f64 	%fd1241, %fd1293, %fd1293;
	mov.f64 	%fd1220, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1222, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1219, %fd1220, %fd1241, %fd1222;
	// end inline asm
	mov.f64 	%fd1226, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1223, %fd1219, %fd1241, %fd1226;
	// end inline asm
	mov.f64 	%fd1230, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1227, %fd1223, %fd1241, %fd1230;
	// end inline asm
	mov.f64 	%fd1234, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1231, %fd1227, %fd1241, %fd1234;
	// end inline asm
	mov.f64 	%fd1238, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1235, %fd1231, %fd1241, %fd1238;
	// end inline asm
	mov.f64 	%fd1242, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1239, %fd1235, %fd1241, %fd1242;
	// end inline asm
	mul.rn.f64 	%fd1307, %fd1239, %fd1241;
	sub.f64 	%fd1308, %fd1245, %fd1293;
	mov.f64 	%fd1309, 0d4000000000000000;
	mul.rn.f64 	%fd1246, %fd1309, %fd1308;
	neg.f64 	%fd1244, %fd1293;
	// begin inline asm
	fma.rn.f64 	%fd1243, %fd1244, %fd1245, %fd1246;
	// end inline asm
	mul.rn.f64 	%fd1289, %fd1305, %fd1243;
	add.f64 	%fd1310, %fd1307, 0d3FB5555555555555;
	mov.f64 	%fd1311, 0d3FB5555555555555;
	sub.f64 	%fd1312, %fd1311, %fd1310;
	add.f64 	%fd1313, %fd1307, %fd1312;
	add.f64 	%fd1314, %fd1313, 0d0000000000000000;
	add.f64 	%fd1315, %fd1314, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1256, %fd1310, %fd1315;
	sub.f64 	%fd1316, %fd1310, %fd1256;
	add.f64 	%fd1260, %fd1315, %fd1316;
	mul.rn.f64 	%fd1317, %fd1256, %fd1293;
	neg.f64 	%fd1250, %fd1317;
	// begin inline asm
	fma.rn.f64 	%fd1247, %fd1256, %fd1293, %fd1250;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1251, %fd1260, %fd1289, %fd1247;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1255, %fd1256, %fd1289, %fd1251;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1259, %fd1260, %fd1293, %fd1255;
	// end inline asm
	add.f64 	%fd1272, %fd1317, %fd1259;
	sub.f64 	%fd1318, %fd1317, %fd1272;
	add.f64 	%fd1276, %fd1259, %fd1318;
	mul.rn.f64 	%fd1319, %fd1272, %fd1293;
	neg.f64 	%fd1266, %fd1319;
	// begin inline asm
	fma.rn.f64 	%fd1263, %fd1272, %fd1293, %fd1266;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1267, %fd1276, %fd1289, %fd1263;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1271, %fd1272, %fd1289, %fd1267;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1275, %fd1276, %fd1293, %fd1271;
	// end inline asm
	add.f64 	%fd1288, %fd1319, %fd1275;
	sub.f64 	%fd1320, %fd1319, %fd1288;
	add.f64 	%fd1292, %fd1275, %fd1320;
	mul.rn.f64 	%fd1321, %fd1288, %fd1293;
	neg.f64 	%fd1282, %fd1321;
	// begin inline asm
	fma.rn.f64 	%fd1279, %fd1288, %fd1293, %fd1282;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1283, %fd1292, %fd1289, %fd1279;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1287, %fd1288, %fd1289, %fd1283;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1291, %fd1292, %fd1293, %fd1287;
	// end inline asm
	add.f64 	%fd1322, %fd1321, %fd1291;
	sub.f64 	%fd1323, %fd1321, %fd1322;
	add.f64 	%fd1324, %fd1291, %fd1323;
	add.f64 	%fd1325, %fd1293, %fd1322;
	sub.f64 	%fd1326, %fd1293, %fd1325;
	add.f64 	%fd1327, %fd1322, %fd1326;
	add.f64 	%fd1328, %fd1324, %fd1327;
	add.f64 	%fd1329, %fd1289, %fd1328;
	add.f64 	%fd1330, %fd1325, %fd1329;
	sub.f64 	%fd1331, %fd1325, %fd1330;
	add.f64 	%fd1332, %fd1329, %fd1331;
	cvt.rn.f64.s32 	%fd1333, %r296;
	mov.f64 	%fd1334, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1335, %fd1333, %fd1334;
	mov.f64 	%fd1336, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1337, %fd1333, %fd1336;
	add.f64 	%fd1338, %fd1335, %fd1330;
	sub.f64 	%fd1339, %fd1335, %fd1338;
	add.f64 	%fd1340, %fd1330, %fd1339;
	add.f64 	%fd1341, %fd1332, %fd1340;
	add.f64 	%fd1342, %fd1337, %fd1341;
	add.f64 	%fd1296, %fd1338, %fd1342;
	sub.f64 	%fd1343, %fd1338, %fd1296;
	add.f64 	%fd1300, %fd1342, %fd1343;
	mul.rn.f64 	%fd1344, %fd1296, %fd1301;
	neg.f64 	%fd1298, %fd1344;
	// begin inline asm
	fma.rn.f64 	%fd1295, %fd1296, %fd1301, %fd1298;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1299, %fd1300, %fd1301, %fd1295;
	// end inline asm
	add.f64 	%fd171, %fd1344, %fd1299;
	sub.f64 	%fd1345, %fd1344, %fd171;
	add.f64 	%fd172, %fd1299, %fd1345;
	mov.f64 	%fd1346, 0d4338000000000000;
	mov.f64 	%fd1347, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1348, %fd171, %fd1347, %fd1346;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r83, %temp}, %fd1348;
	}
	mov.f64 	%fd1349, 0dC338000000000000;
	add.rn.f64 	%fd1350, %fd1348, %fd1349;
	mov.f64 	%fd1351, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1352, %fd1350, %fd1351, %fd171;
	mov.f64 	%fd1353, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1354, %fd1350, %fd1353, %fd1352;
	mov.f64 	%fd1355, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1356, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1357, %fd1356, %fd1354, %fd1355;
	mov.f64 	%fd1358, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1359, %fd1357, %fd1354, %fd1358;
	mov.f64 	%fd1360, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1361, %fd1359, %fd1354, %fd1360;
	mov.f64 	%fd1362, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1363, %fd1361, %fd1354, %fd1362;
	mov.f64 	%fd1364, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1365, %fd1363, %fd1354, %fd1364;
	mov.f64 	%fd1366, 0d3F81111111122322;
	fma.rn.f64 	%fd1367, %fd1365, %fd1354, %fd1366;
	mov.f64 	%fd1368, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1369, %fd1367, %fd1354, %fd1368;
	mov.f64 	%fd1370, 0d3FC5555555555511;
	fma.rn.f64 	%fd1371, %fd1369, %fd1354, %fd1370;
	mov.f64 	%fd1372, 0d3FE000000000000B;
	fma.rn.f64 	%fd1373, %fd1371, %fd1354, %fd1372;
	fma.rn.f64 	%fd1374, %fd1373, %fd1354, %fd1304;
	fma.rn.f64 	%fd1375, %fd1374, %fd1354, %fd1304;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r84, %temp}, %fd1375;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r85}, %fd1375;
	}
	shl.b32 	%r241, %r83, 20;
	add.s32 	%r242, %r85, %r241;
	mov.b64 	%fd1494, {%r84, %r242};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r243}, %fd171;
	}
	mov.b32 	%f12, %r243;
	abs.f32 	%f4, %f12;
	setp.lt.f32 	%p151, %f4, 0f4086232B;
	@%p151 bra 	$L__BB0_157;

	setp.lt.f64 	%p152, %fd171, 0d0000000000000000;
	add.f64 	%fd1376, %fd171, 0d7FF0000000000000;
	selp.f64 	%fd1494, 0d0000000000000000, %fd1376, %p152;
	setp.geu.f32 	%p153, %f4, 0f40874800;
	@%p153 bra 	$L__BB0_157;

	shr.u32 	%r244, %r83, 31;
	add.s32 	%r245, %r83, %r244;
	shr.s32 	%r246, %r245, 1;
	shl.b32 	%r247, %r246, 20;
	add.s32 	%r248, %r85, %r247;
	mov.b64 	%fd1377, {%r84, %r248};
	sub.s32 	%r249, %r83, %r246;
	shl.b32 	%r250, %r249, 20;
	add.s32 	%r251, %r250, 1072693248;
	mov.u32 	%r252, 0;
	mov.b64 	%fd1378, {%r252, %r251};
	mul.f64 	%fd1494, %fd1377, %fd1378;

$L__BB0_157:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r253}, %fd1494;
	}
	and.b32  	%r254, %r253, 2147483647;
	setp.eq.s32 	%p154, %r254, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r255, %temp}, %fd1494;
	}
	setp.eq.s32 	%p155, %r255, 0;
	and.pred  	%p156, %p155, %p154;
	@%p156 bra 	$L__BB0_159;

	// begin inline asm
	fma.rn.f64 	%fd1494, %fd1494, %fd172, %fd1494;
	// end inline asm

$L__BB0_159:
	setp.neu.f64 	%p157, %fd167, 0d3FF0000000000000;
	or.pred  	%p159, %p147, %p157;
	@%p159 bra 	$L__BB0_166;

	mov.b64 	%rd46, %fd1494;
	xor.b64  	%rd47, %rd46, -9223372036854775808;
	mov.b64 	%fd1494, %rd47;
	bra.uni 	$L__BB0_166;

}

  